\documentclass{article}
\usepackage{graphicx} % Required for inserting images
\usepackage{amsmath}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{mathrsfs}
\usepackage{array} % For better table alignment
\usepackage{booktabs}
\usepackage{multirow} % For multi-row cells

\title{Comparison of Frank-Wolfe Varients for White-Box Adversarial Attacks}
\author{Tanner Aaron Graves - 2073559\and Alessandro Pala - 2107800}
\date{June 2024}

\begin{document}

\maketitle 
\section{Abstract}
With deep neural networks becoming ubiquitous in application, adversarial attacks haverecieved much attention, as it has proved remarkably easy to create adversarial examples- genuine data that undergoes a minimal and unobtrusive corruption process inorder to maximally harm the performance of a model. Access to a models architecture can enable white-box attacks, where gradients of loss with respect to input examples are exploided to create adversarial examples. The requirement that examples be minimally preturbed is a constraint on the optimization. The ability of Frank-wolf and vairents have gathered much attention for their ability to be efficiently create adversarial examples while staying in a constraint set. In the paper we introduce discuss the application of Frank-Wolfe and two varients to this non-convex constrained optimization problem. Furthermore we discuss popular optimzations and their effect convergence and attack efficacy, comparing performance on attacks on models trained on MNSIT, FashionMNIST, and CFAIR-10 datasets. Finally, there is a discussion of the theoretical underpinnings of each algorithim.

\section{Introduction}
This is typically stated as the following constrained optimization problem:
\begin{equation}
\begin{aligned}
\min_x \quad & f(x)\\
\text{s.t.} \quad & ||x||_p \leq \epsilon
\end{aligned}
\end{equation}
% targeted / untargeted
In the case of untargeted attacks on a classifier, we perturb an example with the aim it be incorrectly predicted as any other class. Here $f(x)$ is the loss function of the attacked model $-\ell(x, \hat{y})$. In the case of targeted attacks, we aim to maximize the liklihood of another class $y \neq \hat{y}$. The cost then is $f(x) = \ell(x, y)$. We have implemented both targeted and untargeted attacks, and come to focus on targeted attacks as the algorithioms are seen to require more iterations to converge. 
%constraint set
The $L_p$ constraint $||x||_p \leq \epsilon$ directly restricts the size of perturbations made the to the example. An inherient problem of DNNs is often attacks can be consistently sucessful even with very small, even imperceptable $\epsilon$. Different choices of $p$ may be made giving $||x||_p = (\sum_i{x_i^p})^{1/p}$. Or commonly, as we use here $L_\infty(x) = \max_i |x_i|$. 
We define the constraint set $\mathcal{M} = \{x : L_p(x) \leq \epsilon\}$. 
Of pirticular note is when $\mathcal{M}$ is a polytope, as is the case when $p \in \{1, \infty\}$. This corresponds to models making perturbations that are either sparse or have a maximal distubance along each element. In these cases, the constraintset can be expressed as the convex combination of a finite set of verticies $\mathcal{M} = \text{Conv}\{\mathcal{A}\}$. This will be of pirticular relevance in the discussion of Away-step and pairwise varients. Otherwise, $\mathcal{A}$ is taken to be the boundry of $\mathcal{M}$.\break
%Why is constraints an issue
The constrained nature of this problem limits the applicabiliy of a method like gradient descent, and requires integrating knowledge of the constraint space for effective optimization. Methods like Fast Signed Gradient attacks and projected gradient descent are popular choices, but either create unsifisticated adversarial examples or require wasteful prejection onto the constraint space. 

% LMO
We explore Frank-Wolfe variants which are well suited to this problem by ensuring feasibility within the constraint set at each iteration with the efficient solving of a Linear Minimization Oracle (LMO). 
$$
LMO_\mathcal{A}(\nabla f(x_t)) \in \arg \min_{x \in \mathcal{A}} \langle \nabla f(x_t), x\rangle
$$
The LMO is responsible for solving what is called the "Frank-Wolfe Subproblem" and at each iteration provides and optimal $s_t$ such that updating $x_{t+1}$ to move in the direction of $s_t$ will remain in $\mathcal{M}$ Given it moves no more than some maximum stepsize. By solving the LMO efficiently, the algorithm ensures that each iteration makes significant progress towards the optimal solution while respecting the constraints, thereby maintaining feasibility and accelerating convergence. 
% LMO colsed form solutions
Efficient solving of the LMO requires exploiting the structure of the constraint set $\mathcal{M}$. In the case of the $L_\infty$ norm it has closed form solution:
$$LMO_\mathcal{A} = s_t = -\epsilon \text{sign}(\nabla f(x_t)) + x_0$$
%But can be given generally for any choice of $p \in [1, \infty]$.
%$$LMO_\mathcal{A}_i = -\epsilon \text{sign}(\nabla f(x_t))$$
%LMO complexity - FW.pdf
This is clearly of $O(n)$ complexity where $n$ is the number of elements in the gradient. This can be intrepreted as defining an attack direction where each element is the maximum allowable perturbation $\pm \epsilon$ according to the gradient. With one iteration, this is exactly the outcome of a Fast Signed Gradient Attack. At each iteration, we can see that the LMO will give a vertex on the boundry of the constraint set $\mathcal{M}$ which was optimally chosen to be as close to the true gradient as possible while permitting subsequent iterations staty within the feasible set.


% FW GAP as Convergence Criterion
The constrained nature of the Adversarial Attack problem means that the norm of the gradient $||\nabla_x f(x)||$ is not a suitable convergence criterion, as boundary points need not have zero gradient. Instead, we adopt the Frank-Wolfe gap as a measure of both optimality and point feasibility. This gap measures the maximum improvement over the current iteration $x_t$ within the constraints $\mathcal{M}$ and is defined as:
$$g_t^{\text{FW}} := \max_{s \in \mathcal{M}} \langle x_t - s, \nabla f(x_t) \rangle = \langle -\nabla f(x_t), d_t^{\text{FW}} \rangle$$
The Frank-Wolfe gap $g_t^{\text{FW}}$ is always non-negative $g_t^{\text{FW}} \geq 0$ and is zero if and only if $x_t$ is a stationary point. This makes it a useful convergence criterion, particularly since stationary points need not have zero gradients in constrained problems.

The loss functions of Deep Neural Networks (DNNs), which are commonly the subject of adversarial attacks, are highly non-convex. Thus, the convergence of Frank-Wolfe methods in these applications is complicated by the fact that we are not guaranteed to find a global optimum or a successful attack, but rather convergence to a stationary point. 
It is also worth noting that, in the context of adversarial attacks, the Frank-Wolfe gap serves as an imprecise surrogate for success. In many cases, Frank-Wolfe methods generate successful attacks several iterations before convergence. This is because achieving an incorrect class probability higher than the correct class is often sufficient for a successful attack, whereas convergence indicates the new output class probability has been maximized.

\section{Algorithims}
\subsection{Frank-Wolfe}
\begin{algorithm}
\caption{FW for adversarial attacks}\label{alg:cap}
\begin{algorithmic}[1]
\Require maximum iterations $T$, stepsizes $\{\gamma_t\}$, convergence tolerence $\delta$
\Ensure $y = x^n$
\State $x_0 = x_{\text{ori}}$
\For{$t = 1,...,T$}
	% for the sets use the notation from FW_varients for consistency
	% M is the condstrained space of the attack and A is the set of vertieces
	\State $s_t = {\arg \min}_{x\in\mathcal{M}} \langle x, \nabla f(x_t)\rangle$ \Comment{LMO step}
	\State $d_t = s_t - x_t$
	\State $x_{t+1} = x_t + \gamma_t d_t$
	% TODO: maybe mention FW gap converg crit here
	\If{$\langle d_t, -\nabla f(x)\rangle < \delta$} \text{return} \hfill \Comment{FW gap convergence criterion}
	\EndIf
\EndFor
\end{algorithmic}
\end{algorithm}

Observing oscilation in Frank Wolfe convergence is common and consequence of optimal points lying on a face of $\mathcal{M}$. Since at each iteration the method is moving twords a vetex of polytope $\mathcal{M}$, in $\mathcal{S}$, the method "zigzags", moving twords different points in effort to gradually approach the face on which the optimum lies. in the convex case, Frank Wolfe is seen to have linear complexity when optimal point $x^*$ lies in the interior of $\mathcal{M}$, the osicilation causes sublinear convergence when $x^*$ on the boundry.
%citation needed
 We implement varients that aim to address this problem to provide better convergence. The simplest of which is adding momentum to standerd frank wolf which replaces the gradient in the LMO calulation in line $(4)$ with a momentum term $m_t  = \beta m_{t-1} + (1-\beta) \nabla f(x_t)$ and initialize $m_0 = \nabla f(x_0)$. By considering this exponentially weighted average of gradient information, momentum varients are emperically observed to have nicer convergence. 

The FW algorthim has a useful intrepretation that serves as basis for the following varients: Away-Step FW and Pairwise FW. Namely that at each iteration $x_t$ is perturbed in some direction $s_t$, making $x_{t+1}$ a convex combination of $x_t$ and $s_t$. We record these directions in an active set $S_{t+1} = S_t \bigcup \{s_t\}$ and observe that initially $x_0$ is a convex combination of of active set $S_0 = \{x_0\}$. Then by induction, $x_t$ is a convex combination of directions in $S_t$, admitting coefficients $\alpha$ such that $\sum \alpha_{s_i} s_i = x_t$.
Each iteration of the FW algorithim is seen to increase or introduce the contribution of $s_t$ in the convex combination while shrink the $\alpha$ coefficients of all other verticies, or atoms uniformly.
The innovation of the Away-Step and Pairwise varient is to recognize contribution of "bad atoms" can prevent convergence to an optimum on the boundry. These varients more directly diminish such atoms contributions by either taking steps away from selected atoms, or transfering mass between two selected atoms at each iteration as the case with the pairwise varient.

\subsection{Away-Step Frank-Wolfe}
The away-step FW algorithim (AFW) defines a gap in two possible directions: the typical FW step $d_t^\text{FW}$ and an away direction $d_t^\text{A} = x_t - v_t$ where $v_t  \in {\arg \max}_{v\in S_t} \langle v, \nabla f(x_t)\rangle$. By comparing the gaps associated with the two directions, the algorithim compares the cost of taking a typical FW step to the cost associated with moving away from active atoms in $S_t$. Should this gap be less than that of moving twords the frank wolfe direction, the influence of the selected away atom $v_t\in S_t$ will be deminished, and even dropped if the corresponding $\alpha_{v_t} \leq 0$. 
The $\alpha$ updates can be seen to be inverse for that of a normal FW step. Here infulene of $v_t$ is distributed uniformly to other active atoms. New coefficients for the comvex combination are for $v_t$, $\alpha_{v_{t}} := (1+\gamma_t)\alpha_{v_t} - \gamma_t$, and otherwise $\alpha_{v_{t+1}} := (1+\gamma_t)\alpha_{v}$. 
The algorithim enables a "drop step" where the contribution of an atom is compleatly removed from the convex combination, aleviating the problem of osicllation when converging to an optimum on the boundry. 
This algorithim is proved to have linear convergence for convex problems
\begin{algorithm}[H]
\caption{Away-Step FW for Adversarial Attacks}\label{alg:cap}
\begin{algorithmic}[1]
\Require maximum iterations $T$, stepsizes $\{\gamma_t\}$, convergence tolerence $\delta$, $x_0 \in \mathcal{M}$
\State Define $S_0 := \{x_0\}$ with $\alpha_{x_0} = 1$
\For{$t = 1,...,T$}
	\State $s_t  := {\arg \min}_{x\in\mathcal{M}} \langle x, \nabla f(x_t)\rangle$ \Comment{LMO step}
	\State $d_t^{\text{FW}} := s_t - x_t$
	\State $v_t  := {\arg \max}_{v\in S_t} \langle v, \nabla f(x_t)\rangle$
	\State $d_t^{\text{A}} := x_t - v_t$
	% for the sets use the notation from FW_varients for consistency
	% M is the condstrained space of the attack and A is the set of vertieces
	\If{$\langle d_t^\text{FW}, -\nabla f(x)\rangle < \delta$} \text{return} $x_t$ \hfill \Comment{FW gap convergence criterion}
	\EndIf
	\If $\langle d_t^\text{FW}, -\nabla f(x)\rangle < \langle d_t^\text{A}, -\nabla f(x)\rangle$
		\State $d_t = d_t^\text{FW}$, $\gamma_\text{max} := 1$
	\Else
		\State $d_t := d_t^\text{A}$, $\gamma_\text{max} := \frac{\alpha_{v_t}}{1- \alpha_{v_t}}$
	\EndIf
	\State $x_{t+1} = x_t + \gamma_t d_t$
	\State Update $\alpha$, $S_{t+1}$ s.t. $\langle \alpha, S_{t+1}\rangle = x_{t+1}$ (See below)
\EndFor
\end{algorithmic}
\end{algorithm}

\subsection{Pairwise Frank-Wolfe}
Pairwise Frank-Wolfe (PFW) is similar to AFW in that it computes the same away step. However, step taken by the algorithim is not a choice between the two, but rather their sum $d_t^\text{PFW} = d_t^\text{FW} + d_t^\text{A} = s_t - v_t$. 

\begin{algorithm}[H]
\caption{Away-Step FW for Adversarial Attacks}\label{alg:cap}
\begin{algorithmic}[1]
\Require maximum iterations $T$, stepsizes $\{\gamma_t\}$, convergence tolerence $\delta$, $x_0 \in \mathcal{M}$
\State Define $S_0 := \{x_0\}$ with $\alpha_{x_0} = 1$
\For{$t = 1,...,T$}
	\State $s_t  := {\arg \min}_{x\in\mathcal{M}} \langle x, \nabla f(x_t)\rangle$ \Comment{LMO step}
	\State $d_t^{\text{FW}} := s_t - x_t$
	\State $v_t  := {\arg \max}_{v\in S_t} \langle v, \nabla f(x_t)\rangle$
	\If{$\langle d_t^\text{FW}, -\nabla f(x)\rangle < \delta$} \text{return} $x_t$ \hfill \Comment{FW gap convergence criterion}
	\EndIf
	% for the sets use the notation from FW_varients for consistency
	% M is the condstrained space of the attack and A is the set of vertieces
	\State $d_t := s_t - v_t$
	\State $x_{t+1} = x_t + \gamma_t d_t$
	\State Update $\alpha$, $S_{t+1}$ s.t. $\langle \alpha, S_{t+1}\rangle = x_{t+1}$ (See below)
\EndFor
\end{algorithmic}
\end{algorithm}

The behaivor of the of PFW can be understood to be transfering mass in the convex combination from the worst atom to the active set to the atom corresponding the the FW direction. At each iteration the $\alpha$ coefficients are updated as follows: $\alpha_{v_{t+1}} = \alpha_{v_t} - \gamma_t$, $\alpha_{s_{t+1}} = \alpha_{s_t} + \gamma_t$ and all other $\alpha$ values are unchanged. To maintian $x_t$ \emph{convex} combination of atoms in active set, and consequentily not affine or infeasible, its required $\gamma_t \leq \gamma_\text{max} := \alpha_{v_t}$. By directly minimizing, and potentially dropping bad atoms from the active set, the osicilation observed with FW is mitigated. In the convex case this has been shown to give the PFW varient a linear convergence rate.

\section{Results}
Introduce Datasets
\subsection{Momentum}

\subsection{Stepsize}
The methods for stepsize were implemented as follows: Lipschitz constant-based (fixed) stepsize, where the stepsize is determined using the Lipschitz constant \(L\) with \(\gamma_t = \frac{1}{L}\); exact inesearching, which solves the optimization problem \(\arg \min_{\gamma} f(x + \gamma d_t)\) where \(\gamma \in (0,1]\); Decaying stepsize, where the stepsize decreases over time according to \(\gamma_t = \frac{2}{t + 2}\); and Armijo-rule search, which chooses \(\gamma_t\) to satisfy the Armijo rule \(f(x + \gamma_t d_t) \leq f(x) + \delta \gamma_t \nabla f(x)^T d_t\), where \(delta \in (0,1)\) controls the sufficient decrease condition.

\subsection{$\epsilon$ Choice}
Our three different models where pretrained on datasets with differing image scales. To allow for comparison of the sensitivity of each model to attakcks with different $\epsilon$, at each iteration we normalize to ensure images $x_0$ have mean $0.5$ and varience $1$ to ensure equal relative perturbation of the same $\epsilon$ on all three models. 

\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\textwidth]{plots/eps_choice.png}
    \caption{Effect of $\epsilon$ on attack success for untargeted FW for each dataset with $L_\infty$ constraint.}
    \label{fig:converge-compare}
\end{figure}

As figure \ref{fig:converge-compare} shows, the different models have vastly different sensitivities to adversarial attacks. LeNet is the most robust of the three requiring large maximum perturbations for reliable attakcs. The larger, more complex models can be reliably attacked with imperceptible examples in the case of ResNet or borderline perceptible in the case of the FMNIST model. Representitive attacks are shown in figure 2.

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{plots/adv_ex.png}
    \caption{Representative attacks on LeNet, FMNIST, and ResNet 18.}
    \label{fig:adv_ex}
\end{figure}

\subsection{Targeted vs. Untargeted Attacks}
We implemented both untargeted and targeted attacks, requiring defining a custom loss function for the pretrained models. where $f(x) = -\ell(x, y)$ for untargeted attacks, and $f(x) = \ell(x, y_\text{adv})$ where $y$ is the true label and $y_\text{adv}$ the adversarial target class chosen at random to be any class other than $y$. We find, unsuprisingly, that targeted attacks are invariably a more challanging problem, resulting in lower success rates and higher average iterations for all varients and datasets.

For example, attacks on a set of $100$ examples with ResNet18 and $\epsilon = 0.005$ with normal FW algorithim using decaying rule, acheived a success rate of $0.93$ requiring $5.44$ iterations on average in the untargeted case, but a success rate of $0.63$ and $13.26$ iterations in the targeted case.

\subsection{Varients}

% DUMMY VALUES NEED TO BE CHANGED
\begin{table}[H]
    \centering
    \resizebox{\textwidth}{!}{
    \begin{tabular}{@{}lcccccccc@{}}
        \toprule
        \multirow{3}{*}{\textbf{Algorithm}} & \multicolumn{2}{c}{\textbf{MNIST}} & \multicolumn{2}{c}{\textbf{F-MNIST}} & \multicolumn{2}{c}{\textbf{CIFAR-10}} \\
        & \multicolumn{2}{c}{$\epsilon=0.15$} & \multicolumn{2}{c}{$\epsilon=0.025$} & \multicolumn{2}{c}{$\epsilon=0.005$} \\
        \cmidrule(lr){2-3} \cmidrule(lr){4-5} \cmidrule(lr){6-7}
        & \textbf{ASR (\%)} & \textbf{Avg Iter} & \textbf{ASR (\%)} & \textbf{Avg Iter} & \textbf{ASR (\%)} & \textbf{Avg Iter} \\
        \midrule
        FW & 85.4 & 50 & 86.1 & 48 & 87.4 & 10.2 \\
        AFW & 87.3 & 45 & 88.0 & 43 & 86.2 & 10.2 \\
        PFW & 86.5 & 47 & 87.2 & 44 & 82.2 & 11.2 \\
        \bottomrule
	\label{table:varients}
    \end{tabular}
    }
    \caption{Attack Success Rate (ASR) and Average Iterations of Frank-Wolfe Algorithm and Variants for Targeted Adversarial Attacks on 1000 Examples from Three Datasets with $20$ maximum iterations and decaying stepsize rule.}
    \label{tab:algorithm_performance}
\end{table}
We observe that the difficulity of the optimization problem, as influecned by the choice of dataset to attack, $\epsilon$ choice, and perfroming either targeted or untargeted attacks will effect the number of Away Steps taken by AFW. Targeted attacks on CIFAR-10 as described in table 

%REPLACE WITH FINAL RESULTS
\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\textwidth]{plots/mdl_compare_avg_FWgap_by_iter.png}
    \caption{Average $g^\text{FW}_t$ for each Algorithim by Iteration for Targeted Attacks on ResNet-18 (CIFAR-10). $\epsilon = 0.005$.}
    \label{fig:converge-compare}
\end{figure}


\section{Convergence Analysis}
% Proof adaptaed from https://arxiv.org/pdf/1607.00345
% Convergence Rate of Frank-Wolfe for Non-Convex Objectives (Lacoste-Julien 2016)
For the following proof we assume that $f$ has $L$-Lipschitz continuous gradient on $\mathcal{M}$. This is to say that 
$$ f(y) \leq f(x) + \nabla f(x)^T (y-x) + \frac{L}{2}||y-x||_2^2$$.
This has been shown to be a reasonable assumption for DNN. %Santurkar et al. 2018, mentioned in attacks.pdf
Lipshitz continuious gradient gives us bound on curvature constant $C_f$ % (eq5) https://arxiv.org/pdf/1607.00345
Where $C_f$ is the smallest constant such that 
We additionally require that our constraint space $\mathcal{M}$ is convex and compact with dinameter $D$. Satisifying $||x-y||_2 \leq D$ for $x,y \in \mathcal{M}$. This is trivially satisified in the application of adversarial attacks, considering the defintion of $\mathcal{M}$. 
Where Chen et al. 2020 used this strategy to prove the convergence of FW with momentum. We the result holds for normal FW. 
At each iteration since $s_t \in \mathcal{M}$, we get that $||\nabla f(x_t) - s_t||_2$

\subsection{Frank-Wolfe}
\subsection{Pairwise Frank-Wolfe}
\subsection{Away-Step Frank-Wolfe}
\end{document}