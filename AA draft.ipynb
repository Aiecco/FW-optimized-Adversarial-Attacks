{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# PREAMBLE CODE",
   "id": "bcc68a0e37f54cd8"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-06-10T13:12:49.538893Z",
     "start_time": "2024-06-10T13:12:49.533278Z"
    }
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.manual_seed(42)\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from torchinfo import summary\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from IPython.display import clear_output\n",
    "import time"
   ],
   "outputs": [],
   "execution_count": 51
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## DATASETS",
   "id": "852df6a965561589"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### MNIST loader setup",
   "id": "3fbe0f062720b3af"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-10T13:12:49.679761Z",
     "start_time": "2024-06-10T13:12:49.673418Z"
    }
   },
   "cell_type": "code",
   "source": [
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('data', \n",
    "                   train=False, \n",
    "                   download=True, \n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(), \n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])\n",
    "    ),\n",
    "    batch_size=1, shuffle=True)"
   ],
   "id": "cb598ae2061854ce",
   "outputs": [],
   "execution_count": 52
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-10T13:12:49.789177Z",
     "start_time": "2024-06-10T13:12:49.687282Z"
    }
   },
   "cell_type": "code",
   "source": [
    "ex_img = test_loader.dataset[0][0].squeeze()\n",
    "\n",
    "plt.imshow(ex_img, cmap='gray')"
   ],
   "id": "e49752c164f7fc61",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x259713e0c20>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 53
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### FashionMNIST loader setup",
   "id": "4cd484bea1c3726b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "2e3b1cdffadc7d82"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 3rd dataset setup",
   "id": "3a97e3170c08b420"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "7a7dead43a9e85a5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "c04df9b6e1af4d7b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## LeNet (pre-trained) model setup",
   "id": "d27ac8267aa80a72"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-10T13:12:49.801486Z",
     "start_time": "2024-06-10T13:12:49.790184Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class LeNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "        self.dropout1 = nn.Dropout(0.25)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "        self.fc1 = nn.Linear(9216, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "        output = F.log_softmax(x, dim=1)\n",
    "        return output\n",
    "    \n",
    "model = LeNet().to(device)\n",
    "summary(model, input_size=(1, 1, 28, 28))"
   ],
   "id": "8e6a6a2e43afed95",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "LeNet                                    [1, 10]                   --\n",
       "├─Conv2d: 1-1                            [1, 32, 26, 26]           320\n",
       "├─Conv2d: 1-2                            [1, 64, 24, 24]           18,496\n",
       "├─Dropout: 1-3                           [1, 64, 12, 12]           --\n",
       "├─Linear: 1-4                            [1, 128]                  1,179,776\n",
       "├─Dropout: 1-5                           [1, 128]                  --\n",
       "├─Linear: 1-6                            [1, 10]                   1,290\n",
       "==========================================================================================\n",
       "Total params: 1,199,882\n",
       "Trainable params: 1,199,882\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.MEGABYTES): 12.05\n",
       "==========================================================================================\n",
       "Input size (MB): 0.00\n",
       "Forward/backward pass size (MB): 0.47\n",
       "Params size (MB): 4.80\n",
       "Estimated Total Size (MB): 5.27\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 54
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-10T13:12:49.807839Z",
     "start_time": "2024-06-10T13:12:49.801486Z"
    }
   },
   "cell_type": "code",
   "source": [
    "path = r'lenet_mnist_model.pth'\n",
    "model.load_state_dict(torch.load(path, map_location=device))"
   ],
   "id": "da05efcc72d89d31",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 55
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "4c84b3db344994aa"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-10T13:12:49.812836Z",
     "start_time": "2024-06-10T13:12:49.808843Z"
    }
   },
   "cell_type": "code",
   "source": "model.eval()",
   "id": "87a1eb3dea233ed6",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LeNet(\n",
       "  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (dropout1): Dropout(p=0.25, inplace=False)\n",
       "  (dropout2): Dropout(p=0.5, inplace=False)\n",
       "  (fc1): Linear(in_features=9216, out_features=128, bias=True)\n",
       "  (fc2): Linear(in_features=128, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 56
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Some helper functions",
   "id": "2f096ce8a6bb7751"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-10T13:12:49.819100Z",
     "start_time": "2024-06-10T13:12:49.812836Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# from https://pytorch.org/tutorials/beginner/fgsm_tutorial.html\n",
    "\n",
    "def denorm(batch, mean=[0.1307], std=[0.3081]):\n",
    "    \"\"\"\n",
    "    Convert a batch of tensors to their original scale.\n",
    "\n",
    "    Args:\n",
    "        batch (torch.Tensor): Batch of normalized tensors.\n",
    "        mean (torch.Tensor or list): Mean used for normalization.\n",
    "        std (torch.Tensor or list): Standard deviation used for normalization.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: batch of tensors without normalization applied to them.\n",
    "    \"\"\"\n",
    "    if isinstance(mean, list):\n",
    "        mean = torch.tensor(mean).to(device)\n",
    "    if isinstance(std, list):\n",
    "        std = torch.tensor(std).to(device)\n",
    "\n",
    "    return batch * std.view(1, -1, 1, 1) + mean.view(1, -1, 1, 1)"
   ],
   "id": "38b3c2a5fd47d18f",
   "outputs": [],
   "execution_count": 57
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-10T13:12:49.958986Z",
     "start_time": "2024-06-10T13:12:49.819100Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# MODIFED from https://pytorch.org/tutorials/beginner/fgsm_tutorial.html\n",
    "# changelog:\n",
    "# added algo parameter to the function signature and call to the algo function as to define test funct only once and call it with the desired attack algo\n",
    "# added alpha parameter to the function signature as to define test funct only once and call it with the desired step size\n",
    "# added num_iter parameter to the function signature\n",
    "# adapted function returns to fit running function\n",
    "# if statement for FSGM (needs some tinkering cuz the code is too long)\n",
    "\n",
    "def test(model, device, test_loader, epsilon, algo, alpha, num_iter):\n",
    "    if algo == 'FSGM_attack':\n",
    "        correct = 0\n",
    "        adv_examples = []\n",
    "\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            data.requires_grad = True\n",
    "            output = model(data)\n",
    "            init_pred = output.max(1, keepdim=True)[1]\n",
    "\n",
    "            if init_pred.item() != target.item():\n",
    "                continue\n",
    "\n",
    "            loss = F.nll_loss(output, target)\n",
    "            model.zero_grad()\n",
    "            loss.backward()\n",
    "            data_grad = data.grad.data\n",
    "            data_denorm = denorm(data)\n",
    "\n",
    "            perturbed_data = FSGM_attack(data_denorm, epsilon, data_grad)\n",
    "\n",
    "            perturbed_data_normalized = transforms.Normalize((0.1307,), (0.3081,))(perturbed_data)\n",
    "            output = model(perturbed_data_normalized)\n",
    "\n",
    "            final_pred = output.max(1, keepdim=True)[1]\n",
    "            if final_pred.item() == target.item():\n",
    "                correct += 1\n",
    "                if epsilon == 0 and len(adv_examples) < 5:\n",
    "                    adv_ex = perturbed_data.squeeze().detach().cpu().numpy()\n",
    "                    adv_examples.append((init_pred.item(), final_pred.item(), adv_ex))\n",
    "            else:\n",
    "                if len(adv_examples) < 5:\n",
    "                    adv_ex = perturbed_data.squeeze().detach().cpu().numpy()\n",
    "                    adv_examples.append((init_pred.item(), final_pred.item(), adv_ex))\n",
    "\n",
    "        final_acc = correct / float(len(test_loader))\n",
    "        print(f\"Epsilon: {epsilon}\\tTest Accuracy = {correct} / {len(test_loader)} = {final_acc}\")\n",
    "\n",
    "        return final_acc, adv_examples\n",
    "\n",
    "    else:\n",
    "        correct = 0\n",
    "        adv_examples = []\n",
    "        iteration_accuracies = []\n",
    "        step_sizes = []\n",
    "        iteration_times = []\n",
    "\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            data.requires_grad = True\n",
    "            output = model(data)\n",
    "            init_pred = output.max(1, keepdim=True)[1]\n",
    "\n",
    "            if init_pred.item() != target.item():\n",
    "                continue\n",
    "\n",
    "            loss = F.nll_loss(output, target)\n",
    "            model.zero_grad()\n",
    "            loss.backward()\n",
    "            data_grad = data.grad.data\n",
    "            data_denorm = denorm(data)\n",
    "\n",
    "            start_time = time.time()\n",
    "            if algo == PGD_const:\n",
    "                perturbed_data = PGD_const(model, data_denorm, target, epsilon, alpha, num_iter)\n",
    "            else:\n",
    "                perturbed_data = algo(model, data_denorm, target, epsilon, num_iter, alpha)\n",
    "            iteration_time = time.time() - start_time\n",
    "\n",
    "            perturbed_data_normalized = transforms.Normalize((0.1307,), (0.3081,))(perturbed_data)\n",
    "            output = model(perturbed_data_normalized)\n",
    "            final_pred = output.max(1, keepdim=True)[1]\n",
    "\n",
    "            if final_pred.item() == target.item():\n",
    "                correct += 1\n",
    "                if epsilon == 0 and len(adv_examples) < 5:\n",
    "                    adv_ex = perturbed_data.squeeze().detach().cpu().numpy()\n",
    "                    adv_examples.append((init_pred.item(), final_pred.item(), adv_ex))\n",
    "            else:\n",
    "                if len(adv_examples) < 5:\n",
    "                    adv_ex = perturbed_data.squeeze().detach().cpu().numpy()\n",
    "                    adv_examples.append((init_pred.item(), final_pred.item(), adv_ex))\n",
    "\n",
    "            iteration_accuracies.append(correct / float(len(test_loader)))\n",
    "            step_sizes.append(epsilon)\n",
    "            iteration_times.append(iteration_time)\n",
    "\n",
    "        final_acc = correct / float(len(test_loader))\n",
    "        print(f\"Epsilon: {epsilon}\\tTest Accuracy = {correct} / {len(test_loader)} = {final_acc}\")\n",
    "\n",
    "        return final_acc, adv_examples, iteration_accuracies, step_sizes, iteration_times"
   ],
   "id": "7fede79e9040c15b",
   "outputs": [],
   "execution_count": 58
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-10T13:12:49.966296Z",
     "start_time": "2024-06-10T13:12:49.958986Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def plot_metrics(epsilons, accuracies, iteration_accuracies, iteration_times, step_sizes):\n",
    "    fig, axs = plt.subplots(4, 1, figsize=(10, 24))\n",
    "    \n",
    "    sns.lineplot(ax=axs[0], x=epsilons, y=accuracies, label=\"Accuracy vs Epsilon\", linewidth=0.75, errorbar=None)\n",
    "    axs[0].set_xlabel('Epsilon', fontsize=14)\n",
    "    axs[0].set_ylabel('Accuracy', fontsize=14)\n",
    "    axs[0].set_title(\"Accuracy vs Epsilon\", fontsize=16)\n",
    "\n",
    "    sns.lineplot(ax=axs[1], x=np.arange(len(iteration_accuracies)), y=iteration_accuracies)\n",
    "    axs[1].set_title('Accuracy vs Iteration')\n",
    "    axs[1].set_xlabel('Iteration')\n",
    "    axs[1].set_ylabel('Accuracy')\n",
    "\n",
    "    sns.lineplot(ax=axs[2], x=np.arange(len(iteration_times)), y=iteration_times)\n",
    "    axs[2].set_title('Accuracy vs Time')\n",
    "    axs[2].set_xlabel('Iteration')\n",
    "    axs[2].set_ylabel('Time (s)')\n",
    "\n",
    "    sns.lineplot(ax=axs[3], x=np.arange(len(step_sizes)), y=step_sizes)\n",
    "    axs[3].set_title('Step Size Behavior')\n",
    "    axs[3].set_xlabel('Iteration')\n",
    "    axs[3].set_ylabel('Step Size')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ],
   "id": "e5ea74b9e012e6bb",
   "outputs": [],
   "execution_count": 59
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-10T13:12:49.973274Z",
     "start_time": "2024-06-10T13:12:49.967304Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# MODIFED from https://pytorch.org/tutorials/beginner/fgsm_tutorial.html\n",
    "# changelog: made it a function instead of the whole code so that we slim the code\n",
    "\n",
    "def plot_examples(examples, epsilons):\n",
    "    cnt = 0\n",
    "    plt.figure(figsize=(8, 10))\n",
    "    for i in range(len(epsilons)):\n",
    "        for j in range(len(examples[i])):\n",
    "            cnt += 1\n",
    "            plt.subplot(len(epsilons), len(examples[0]), cnt)\n",
    "            plt.xticks([], [])\n",
    "            plt.yticks([], [])\n",
    "            if j == 0:\n",
    "                plt.ylabel(f\"Eps: {epsilons[i]}\", fontsize=14)\n",
    "            orig, adv, ex = examples[i][j]\n",
    "            plt.title(f\"{orig} -> {adv}\")\n",
    "            plt.imshow(ex, cmap=\"gray\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ],
   "id": "927ac72f3d07fb",
   "outputs": [],
   "execution_count": 60
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-10T13:12:49.980503Z",
     "start_time": "2024-06-10T13:12:49.973274Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# function runs the attack for each epsilon and plots the accuracy, and at the end plots the examples\n",
    "\n",
    "def RUN(model, device, test_loader, algo, num_iter, epsilons, alpha):\n",
    "    accuracies, examples, all_iteration_accuracies, all_step_sizes, all_iteration_times = [], [], [], [], []\n",
    "\n",
    "    for eps in epsilons:\n",
    "        acc, ex, iteration_accuracies, step_sizes, iteration_times = test(model, device, test_loader, eps, algo, alpha, num_iter)\n",
    "        accuracies.append(acc)\n",
    "        examples.append(ex)\n",
    "        all_iteration_accuracies.extend(iteration_accuracies)\n",
    "        all_step_sizes.extend(step_sizes)\n",
    "        all_iteration_times.extend(iteration_times)\n",
    "\n",
    "    plot_examples(examples, epsilons)\n",
    "    plot_metrics(epsilons, accuracies, all_iteration_accuracies, all_iteration_times, all_step_sizes)"
   ],
   "id": "47b2df81ad7a564",
   "outputs": [],
   "execution_count": 61
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Optimization ''hyperparams'' setup",
   "id": "206e2997e467b290"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Perturbation set bounds",
   "id": "61bcd3567664962a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-10T13:12:49.986993Z",
     "start_time": "2024-06-10T13:12:49.981510Z"
    }
   },
   "cell_type": "code",
   "source": "epsilons = [0, .05, .1, .15, .2, .25, .3, .35]",
   "id": "9aa8b4be7842190c",
   "outputs": [],
   "execution_count": 62
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Number of iterations",
   "id": "d121465081ed304c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-10T13:12:49.993636Z",
     "start_time": "2024-06-10T13:12:49.986993Z"
    }
   },
   "cell_type": "code",
   "source": "num_iter = 40",
   "id": "778109563600441f",
   "outputs": [],
   "execution_count": 63
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Constant step sizes",
   "id": "c6f8affa9c2f6510"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Unit step size",
   "id": "f93ab77fe3a82a45"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-10T13:12:50.000076Z",
     "start_time": "2024-06-10T13:12:49.993636Z"
    }
   },
   "cell_type": "code",
   "source": "alpha_unit = 1",
   "id": "4866de30cfdee6fc",
   "outputs": [],
   "execution_count": 64
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Lipschitz constant dependent step size",
   "id": "6b1a0fc3b4e00056"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-10T13:12:50.047723Z",
     "start_time": "2024-06-10T13:12:50.000076Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def lipschitz_constant(model):\n",
    "    L = 0  # initialize\n",
    "\n",
    "    for layer in model.children():\n",
    "        if isinstance(layer, nn.Conv2d) or isinstance(layer, nn.Linear):\n",
    "            \n",
    "            weight = layer.weight.data # extract weight data\n",
    "            \n",
    "            singular_values = torch.svd(weight).S     # as we did initially in the homework, we approximate the Lipschitz\n",
    "            L = max(L, singular_values.max().item())  # constant as the maximum singular value of the weight matrix\n",
    "    \n",
    "    return L\n",
    "\n",
    "L = lipschitz_constant(model)\n",
    "\n",
    "alpha_lipschitz = 1 / L\n",
    "\n",
    "alpha_lipschitz"
   ],
   "id": "928bea6b955a5a3f",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.14922538993069884"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 65
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Adaptive step sizes",
   "id": "5a4240a4b87ad7db"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Diminishing step size",
   "id": "6829a2823d146635"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-10T13:12:50.050910Z",
     "start_time": "2024-06-10T13:12:50.047723Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def alpha_dim(num_iter):\n",
    "    alphas = [2 / (k + 2) for k in range(num_iter)] # defined as a list cuz it's easier to call the i-th element\n",
    "    return alphas"
   ],
   "id": "7c1a6166133a94b2",
   "outputs": [],
   "execution_count": 66
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "##### Exact line search\n",
    "\n",
    "From Rinaldi's survey paper:\n",
    "\n",
    "The exact line search method aims to find the optimal step size $ \\( \\alpha \\)$  that minimizes the objective function along the direction of the gradient descent.\n",
    "\n",
    "$ \\[ \\alpha_k = \\min_{\\alpha \\in [0, \\alpha_{\\text{max}}^k]} \\varphi(\\alpha) \\] $, where:\n",
    "\n",
    "$ \\[ \\varphi(\\alpha) = f(x_k + \\alpha d_k) \\]$ \n",
    "\n",
    "- $\\( x_k \\)$: Current point in the parameter space.\n",
    "- $\\( d_k \\)$: Descent direction (typically the negative gradient).\n",
    "- $\\( \\alpha_{\\text{max}}^k \\)$: Maximum allowable step size.\n",
    "- $\\( \\varphi(\\alpha) \\)$: Objective function along the direction $\\( d_k \\)$ with step size $\\( \\alpha \\)$.\n",
    "\n",
    "The goal is to find the smallest $\\( \\alpha \\)$ that minimizes $\\( \\varphi(\\alpha) \\)$."
   ],
   "id": "6cf0aba7550f4534"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-10T13:12:50.057497Z",
     "start_time": "2024-06-10T13:12:50.050910Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# define initial step size\n",
    "\n",
    "alpha_max = 0.01 # for adversarial attacks on images with pixel values normalized between 0 and 1, α_max = 0.1 or α_max = 0.01 is a common choice."
   ],
   "id": "165878b37ac4fd55",
   "outputs": [],
   "execution_count": 67
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-10T13:12:50.064480Z",
     "start_time": "2024-06-10T13:12:50.057497Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def exact_LS(model, image, target, data_grad, num_iter):\n",
    "    def objective_function(alpha):\n",
    "        perturbed_image = image + alpha * data_grad\n",
    "        perturbed_image = torch.clamp(perturbed_image, 0, 1)\n",
    "        output = model(perturbed_image)\n",
    "        loss = F.cross_entropy(output, target)\n",
    "        return loss.item()\n",
    "\n",
    "    alpha_range = torch.linspace(0, alpha_max, num_iter)\n",
    "    losses = [objective_function(alpha) for alpha in alpha_range]\n",
    "    best_alpha = alpha_range[torch.argmin(torch.tensor(losses))]\n",
    "\n",
    "    return best_alpha"
   ],
   "id": "5d7328428576fe36",
   "outputs": [],
   "execution_count": 68
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "##### Armijo rule\n",
    "\n",
    "From Rinaldi's survey paper:\n",
    "\n",
    "\n",
    "$\\[ \\alpha_k = \\min_{\\alpha \\in [0, \\alpha_{\\text{max}}^k]} \\varphi(\\alpha) \\]$, where:\n",
    "\n",
    "$\\[ \\varphi(\\alpha) = f(x_k + \\alpha d_k) \\]$\n",
    "\n",
    "- $\\( x_k \\)$: Current point in the parameter space.\n",
    "- $\\( d_k \\)$: Descent direction (typically the negative gradient).\n",
    "- $\\( \\alpha_{\\text{max}}^k \\)$: Maximum allowable step size.\n",
    "- $\\( \\varphi(\\alpha) \\)$: Objective function along the direction $\\( d_k \\)$ with step size $\\( \\alpha \\)$.\n",
    "\n",
    "The goal is to find the smallest $\\( \\alpha \\)$ that minimizes $\\( \\varphi(\\alpha) \\)$."
   ],
   "id": "170e6af1a0538544"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-10T13:12:50.071096Z",
     "start_time": "2024-06-10T13:12:50.064480Z"
    }
   },
   "cell_type": "code",
   "source": [
    "delta = 0.5\n",
    "gamma = 0.1\n",
    "M = 1000 \n",
    "\n",
    "def armijo(model, image, target, data_grad):\n",
    "    alpha = alpha_max\n",
    "    m = 0\n",
    "\n",
    "    while m < M:\n",
    "        perturbed_image = image + alpha * data_grad\n",
    "        perturbed_image = torch.clamp(perturbed_image, 0, 1)\n",
    "        output = model(perturbed_image)\n",
    "        loss = F.cross_entropy(output, target)\n",
    "        \n",
    "        if loss.item() <= F.cross_entropy(model(image), target).item() + gamma * alpha * data_grad.norm():\n",
    "            break\n",
    "        else:\n",
    "            alpha *= delta\n",
    "            m += 1\n",
    "\n",
    "    return alpha"
   ],
   "id": "868dbb5fa091c26d",
   "outputs": [],
   "execution_count": 69
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "1ceafc6147155327"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Non-FW Attacks",
   "id": "c99182d5c90c8340"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Attack PGD",
   "id": "7af2bc044aaed00e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-10T13:12:50.077702Z",
     "start_time": "2024-06-10T13:12:50.072103Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# could not slim the code to only one function\n",
    "# as for constant step sizes, the attack follows an algo structure that requires a function arg that should not be passed in adaptive step sizes algos\n",
    "# so at most I could define two functions for each algo, one for constant step sizes and one for adaptive step sizes"
   ],
   "id": "b5f03216d529fa53",
   "outputs": [],
   "execution_count": 70
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-10T13:12:50.084409Z",
     "start_time": "2024-06-10T13:12:50.077702Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def PGD_const(model, image, target, epsilon, alpha, num_iter):\n",
    "    perturbed_image = image.clone().detach().requires_grad_(True)\n",
    "    for _ in range(num_iter):\n",
    "        # Zero all existing gradients\n",
    "        perturbed_image.requires_grad = True\n",
    "\n",
    "        # Forward pass the data through the model\n",
    "        output = model(perturbed_image)\n",
    "        loss = torch.nn.functional.cross_entropy(output, target)\n",
    "            \n",
    "        # Zero all existing gradients\n",
    "        model.zero_grad()\n",
    "\n",
    "        # Calculate gradients of model in backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Collect the element-wise sign of the data gradient\n",
    "        data_grad = perturbed_image.grad.data\n",
    "        \n",
    "        # call the constant step size\n",
    "        if alpha == 'alpha_unit':\n",
    "            best_alpha = alpha_unit\n",
    "        elif alpha == 'alpha_lipschitz':\n",
    "            best_alpha = alpha_lipschitz\n",
    "\n",
    "        perturbed_image = perturbed_image + best_alpha * data_grad.sign() # move in the direction of the gradient\n",
    "        perturbation = torch.clamp(perturbed_image - image, -epsilon, epsilon) # clip to stay within the given epsilon\n",
    "        perturbed_image = torch.clamp(image + perturbation, 0, 1).detach() # apply perturbation and clip within valid pixel range (we de-normed to [0,1])\n",
    "\n",
    "    # Return the final perturbed image after all iterations\n",
    "    return perturbed_image"
   ],
   "id": "107108c598b3f8f",
   "outputs": [],
   "execution_count": 71
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-10T13:12:50.091357Z",
     "start_time": "2024-06-10T13:12:50.084409Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def PGD_adapt(model, image, target, epsilon, num_iter, search_method):\n",
    "    perturbed_image = image.clone().detach().requires_grad_(True)\n",
    "\n",
    "    for i in range(num_iter):\n",
    "        perturbed_image.requires_grad = True\n",
    "\n",
    "        output = model(perturbed_image)\n",
    "        loss = F.cross_entropy(output, target)\n",
    "\n",
    "        model.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        data_grad = perturbed_image.grad.data\n",
    "\n",
    "        if search_method == \"exact\":\n",
    "            best_alpha = exact_LS(model, perturbed_image, target, data_grad, num_iter)\n",
    "        elif search_method == \"armijo\":\n",
    "            best_alpha = armijo(model, perturbed_image, target, data_grad)\n",
    "        elif search_method == \"alpha_dim\":\n",
    "            alphas = alpha_dim(num_iter)\n",
    "            best_alpha = alphas[i] # # use the precomputed alpha value (since in this case it's only dependent on the number of iterations)\n",
    "\n",
    "        perturbed_image = perturbed_image + best_alpha * data_grad.sign() # move in the direction of the gradient\n",
    "        perturbation = torch.clamp(perturbed_image - image, -epsilon, epsilon) # clip to stay within the given epsilon\n",
    "        perturbed_image = torch.clamp(image + perturbation, 0, 1).detach() # apply perturbation and clip within valid pixel range (we de-normed to [0,1])\n",
    "\n",
    "    return perturbed_image"
   ],
   "id": "781757aa0b8f7155",
   "outputs": [],
   "execution_count": 72
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Attack PGD unit step size",
   "id": "daeca8507f42543"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2024-06-10T13:12:50.114364Z"
    }
   },
   "cell_type": "code",
   "source": "RUN(model, device, test_loader, PGD_const, num_iter, epsilons, 'alpha_unit')",
   "id": "6179f2f710bcc386",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epsilon: 0\tTest Accuracy = 9912 / 10000 = 0.9912\n",
      "Epsilon: 0.05\tTest Accuracy = 9765 / 10000 = 0.9765\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Attack PGD lipschitz constant dependent step size",
   "id": "f0e10ca4b831d6b3"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": "RUN(model, device, test_loader, PGD_const, num_iter, epsilons, 'alpha_lipschitz')",
   "id": "aa55a7b0152a0c1e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Attack PGD diminishing step size",
   "id": "549a639a8b640967"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": "RUN(model, device, test_loader, PGD_adapt, num_iter, \"alpha_dim\")",
   "id": "bb8103f35f1224f5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Attack PGD exact line search",
   "id": "37625475a2d7a242"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": "RUN(model, device, test_loader, PGD_adapt, num_iter, \"exact\")",
   "id": "8683a50f0d893837",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Attack PGD armijo rule",
   "id": "74e2b3c869f65b6c"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": "RUN(model, device, test_loader, PGD_adapt, num_iter, \"armijo\")",
   "id": "b1627d3f244153c0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Attack FSGM",
   "id": "2afc0e38ff39b481"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "# MODIFIED from https://pytorch.org/tutorials/beginner/fgsm_tutorial.html\n",
    "# no step size since it is not an iterative method\n",
    "# changelog:\n",
    "# added model, target, data_grad, alpha and num_iter parameters to the function signature to fit it to the test function\n",
    "\n",
    "# FGSM attack code\n",
    "def FSGM_attack(image, epsilon, data_grad):\n",
    "    # Collect the element-wise sign of the data gradient\n",
    "    sign_data_grad = data_grad.sign()\n",
    "    # Create the perturbed image by adjusting each pixel of the input image\n",
    "    perturbed_image = image + epsilon*sign_data_grad\n",
    "    # Adding clipping to maintain [0,1] range\n",
    "    perturbed_image = torch.clamp(perturbed_image, 0, 1)\n",
    "    # Return the perturbed image\n",
    "    return perturbed_image"
   ],
   "id": "7832b9eb42cbf0f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "# since it is not an iterative model, we can't plot time per iteration and accuracy per iteration metrics, so we'll use raw code\n",
    "# MODIFIED from https://pytorch.org/tutorials/beginner/fgsm_tutorial.html\n",
    "# changelog:\n",
    "# fit the function to the test function's if statement, with of course None as step size and number of iterations\n",
    "## test function may be further slimmed by avoiding some repetition putting the if statement somewhere else, but it's not a big deal\n",
    "\n",
    "accuracies = []\n",
    "examples = []\n",
    "\n",
    "# Run test for each epsilon\n",
    "def RUN_FSGM(model, device, test_loader, epsilons):\n",
    "    for eps in epsilons:\n",
    "        acc, ex = test(model, device, test_loader, eps, 'FSGM_attack', None, None)\n",
    "        accuracies.append(acc)\n",
    "        examples.append(ex)\n",
    "        \n",
    "    plt.figure(figsize=(5,5))\n",
    "    plt.plot(epsilons, accuracies, \"*-\")\n",
    "    plt.yticks(np.arange(0, 1.1, step=0.1))\n",
    "    plt.xticks(np.arange(0, .35, step=0.05))\n",
    "    plt.title(\"Accuracy vs Epsilon\")\n",
    "    plt.xlabel(\"Epsilon\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.show()    \n",
    "    \n",
    "    plot_examples(examples, epsilons)\n",
    "    \n",
    "RUN_FSGM(model, device, test_loader, epsilons)"
   ],
   "id": "72e4fb09193933b4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# FW (and variants) Attacks",
   "id": "2ff1f6b9a19b01a5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "# merge tanner code",
   "id": "55103f50f9b37072"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
