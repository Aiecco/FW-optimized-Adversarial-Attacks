{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# PREAMBLE CODE",
   "id": "bcc68a0e37f54cd8"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-06-10T20:24:29.259104Z",
     "start_time": "2024-06-10T20:24:29.254530Z"
    }
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.manual_seed(42)\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from models.simple_FashionMNIST import simple_FashionMNIST\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import ace as tools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from IPython.display import clear_output\n",
    "import time"
   ],
   "outputs": [],
   "execution_count": 85
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## DATASETS",
   "id": "852df6a965561589"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### MNIST loader setup",
   "id": "3fbe0f062720b3af"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-10T20:24:29.377473Z",
     "start_time": "2024-06-10T20:24:29.371280Z"
    }
   },
   "cell_type": "code",
   "source": [
    "mnist_test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('data', \n",
    "                   train=False, \n",
    "                   download=True, \n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(), \n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])\n",
    "    ),\n",
    "    batch_size=1, shuffle=True)"
   ],
   "id": "cb598ae2061854ce",
   "outputs": [],
   "execution_count": 86
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-10T20:24:29.480242Z",
     "start_time": "2024-06-10T20:24:29.378482Z"
    }
   },
   "cell_type": "code",
   "source": [
    "mnist_ex_img = mnist_test_loader.dataset[0][0].squeeze()\n",
    "plt.imshow(mnist_ex_img, cmap='gray')"
   ],
   "id": "e49752c164f7fc61",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x25f24dbbd10>"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ],
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZ4AAAGdCAYAAAAi6BWhAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy80BEi2AAAACXBIWXMAAA9hAAAPYQGoP6dpAAAeL0lEQVR4nO3df2xV9f3H8dct2F/21gqtBJpNVmIRFGmtA1S6rN/BIIADlIzFGa1E3B9Il6xBBgPnhIxAI4hTUMcPiSDgJHFqDCpkU1FBJAMCCra3GrCEpkzBWyit0PP944abXbj9ca6n79vb+3wkN+Z+Pv2c8/bd074459576nMcxxEAAEZS4l0AACC5EDwAAFMEDwDAFMEDADBF8AAATBE8AABTBA8AwBTBAwAwRfAAAEwRPAAAU7293Fhzc7P+8pe/6J133lF6erpmzJihGTNmuNpGfn6+gsFgxJjf71ddXV3UuWRCH0LoQwh9CKEPId2hD5dq6IinwbNs2TIdOnRIGzZs0IkTJzR37lwNGDBA48eP7/Q2gsFgm01rby6Z0IcQ+hBCH0LoQ0gi9MGz4Dl37pz+8Y9/6O9//7tuuukm3XTTTaqurtamTZtcBQ8AoGfz7DWeI0eO6MKFCyouLg6PlZSU6MCBA2ptbfVqNwCABOfZGU9DQ4OuvfZapaamhsdyc3PV3Nys06dPq0+fPp3ajt/vb3Ms2lwyoQ8h9CGEPoTQh5Du0IfO7tuz4GlqaooIHUnh5y0tLZ3eTnsvTHXmRatkQB9C6EMIfQihDyGJ0AfPgictLe2KgLn0PD09vdPb4V1tbaMPIfQhhD6E0IeQ7tAH83e19evXT99++60uXLig3r1Dm21oaFB6erqys7M7vR3e1dYx+hBCH0LoQwh9CEmEPnj25oIhQ4aod+/e2r9/f3hs3759GjZsmFJS+JwqACDEs0TIyMjQlClT9Pjjj+vgwYPasWOH1q1bp/vvv9+rXQAAegBPP0A6b948Pf7443rggQeUlZWl2bNn65e//KWXuwAAJDhPgycjI0NLly7V0qVLvdwsAKAH4cUXAIApggcAYIrgAQCYIngAAKYIHgCAKYIHAGCK4AEAmCJ4AACmCB4AgCmCBwBgiuABAJgieAAApggeAIApggcAYIrgAQCYIngAAKYIHgCAKYIHAGCK4AEAmCJ4AACmCB4AgCmCBwBgiuABAJgieAAApggeAIApggcAYIrgAQCYIngAAKYIHgCAKYIHAGCK4AEAmCJ4AACmCB4AgCmCBwBgiuABAJgieAAApggeAIApggcAYIrgAQCYIngAAKYIHgCAKYIHAGCK4AEAmCJ4AACmCB4AgCmCBwBgiuABAJgieAAApggeAIApggcAYIrgAQCYIngAAKYIHgCAKYIHAGDK0+B59913NXjw4IhHRUWFl7sAACS43l5urKamRmVlZVq0aFF4LC0tzctdAAASnKfBEwgEVFhYqLy8PC83CwDoQTy91BYIBDRw4EAvNwkA6GE8O+NxHEdffvmldu3apeeff14XL17U+PHjVVFRodTU1E5vx+/3tzkWbS6Z0IcQ+hBCH0LoQ0h36ENn9+1zHMfxYod1dXX6v//7P02dOlUPPPCAvv76ay1evFhjx47VggULvNgFAKAH8Cx4JOn06dO65ppr5PP5JElvv/225syZo//85z/q1atXp7aRn5+vYDAYMeb3+1VXVxd1LpnQhxD6EEIfQuhDSHfow6UaOuLpmwtycnIing8aNEjNzc06c+aM+vTp06ltBIPBNpvW3lwyoQ8h9CGEPoTQh5BE6INnby744IMPNHLkSDU1NYXHPv/8c+Xk5HQ6dAAAPZ9nwVNcXKy0tDQtWLBAtbW1eu+997Rs2TI99NBDXu0CANADeHapLSsrS2vXrtVf//pX3XPPPbr66qv1m9/8huABAETw9DWeG264QevXr/dykwCAHoabhAIATBE8AABTBA8AwBTBAwAwRfAAAEwRPAAAUwQPAMAUwQMAMEXwAABMETwAAFMEDwDAFMEDADDl6U1CET/Tpk1zvWbmzJkx7evEiROu15w/f971mk2bNkUdz8zMlCTdfvvtOnfuXMTcyZMnXe9HkmpqamJaB8A9zngAAKYIHgCAKYIHAGCK4AEAmCJ4AACmCB4AgCmCBwBgiuABAJgieAAApggeAIApggcAYIrgAQCYIngAAKa4O3UPsWzZMtdrBg4c6H0hHvrd737X7vz27duvGAsGgzHt6/DhwzGti6devXpJknbs2KGLFy/GuZqu9/XXX0cd79079Gtsw4YNunDhQsRcLD8XkvTpp5/GtA6dwxkPAMAUwQMAMEXwAABMETwAAFMEDwDAFMEDADBF8AAATBE8AABTBA8AwBTBAwAwRfAAAEwRPAAAU9wktIeYOXOm6zW33HJLTPv6/PPPXa8ZMmSI6zW33npr1PGrrrpK06dP1yuvvKLvv/8+Yu7nP/+56/1I0qhRo1yvOX78uOs1P/rRj1yv6chPf/pTz7Z1+U02O6OhocH1mv79+7te05EpU6ZcMXbs2LGYtsVNQrsWZzwAAFMEDwDAFMEDADBF8AAATBE8AABTBA8AwBTBAwAwRfAAAEwRPAAAUwQPAMAUwQMAMEXwAABMcZPQHmLnzp0ma2K1fft2z7bl9/s1ffp0zZw5U8FgMGLu2muvjWmbRUVFrtfs27fP9Rovb+iZmZmp119/XZMnT9a5c+c82eb58+ddr/niiy9cr4nlRrN9+vRxvSYQCLheg67HGQ8AwFTMwdPS0qJJkyZpz5494bHjx4+rvLxcRUVFmjBhgnbt2uVJkQCAniOm4GlubtYf/vAHVVdXh8ccx9GsWbOUm5urbdu2afLkyXrkkUd04sQJz4oFACQ+16/x1NTUqLKyUo7jRIzv3r1bx48f15YtW5SZmalBgwbp448/1rZt2zR79mzPCgYAJDbXZzyffPKJRo4cqa1bt0aMHzhwQEOHDlVmZmZ4rKSkRPv37//BRQIAeg7XZzz33ntv1PGGhgZdd911EWN9+/bVyZMnXW3f7/e3ORZtLpnQh5D2+hBrb/73H0xu6+jq/bQlIyMj4r9eSElxf/U9KyvL9Rqfz+d6TSzS09NjWpeIP2Pd4fdDZ/ft2dupm5qalJqaGjGWmpqqlpYWV9upq6uLaS6Z0IcQ+hBy+dWHZBUtzJYvXx7TtmJd1x0kws+FZ8GTlpam06dPR4y1tLS4/hdHfn7+FZ/N8Pv9qqurizqXTOhDSHt9iPVzPMOGDXO9JpbLyLfeeqvrNW3JyMjQ1q1bNX36dDU1NXmyzVg+x1NTU+N6zaeffup6TXvfW5/Pd8XrzpJUWVnpej+StGbNmpjWxVN3+P1wqYaOeBY8/fr1u+IAPHXq1BWX3zoSDAbbbFp7c8mEPoRE60Pv3rEd0rF8ADOW74FXH/T8X01NTXH9AGljY6PrNdFCoivE8v8jxfa97S4S4feDZx8gHT58uA4fPhzxjd63b5+GDx/u1S4AAD2AZ8EzYsQI9e/fX/PmzVN1dbVeeOEFHTx4UNOmTfNqFwCAHsCz4OnVq5dWrVqlhoYG3X333Xr99df17LPPasCAAV7tAgDQA/gcq4utnZSdnR31zQXfffdd1LlkQh9C6ENId+nDPffc43rNK6+84nrNoUOHoo6npKTo5ptv1qFDh9Ta2hoxV1ZW5no/kvTNN9/EtC6eusPxcKmGjnCTUACAKYIHAGCK4AEAmCJ4AACmCB4AgCmCBwBgiuABAJgieAAApggeAIApggcAYIrgAQCYIngAAKYIHgCAKc/+AimAxOf2LwZL0qpVq1yvSUlx/2/eJ554Iup4RkaGXnrpJS1duvSKPwGeiHeZTgac8QAATBE8AABTBA8AwBTBAwAwRfAAAEwRPAAAUwQPAMAUwQMAMEXwAABMETwAAFMEDwDAFMEDADDFTUIBhM2aNcv1mry8PNdrvv32W9drjh49GnX86quvliRVV1fr7NmzrrcLe5zxAABMETwAAFMEDwDAFMEDADBF8AAATBE8AABTBA8AwBTBAwAwRfAAAEwRPAAAUwQPAMAUwQMAMMVNQoEe6M4774xp3R//+EePK4luypQprtccOnQo6rjf75ckffbZZwoGgz+kLBjhjAcAYIrgAQCYIngAAKYIHgCAKYIHAGCK4AEAmCJ4AACmCB4AgCmCBwBgiuABAJgieAAApggeAIApbhIK9EATJkyIad1VV13les3OnTtdr/n4449dr0HPwRkPAMBUzMHT0tKiSZMmac+ePeGxxYsXa/DgwRGPjRs3elIoAKBniOlSW3NzsyorK1VdXR0xHggEVFlZqalTp4bHsrKyfliFAIAexfUZT01NjX7961/r2LFjV8wFAgENHTpUeXl54UdGRoYnhQIAegbXwfPJJ59o5MiR2rp1a8R4Y2Oj6uvrNXDgQK9qAwD0QK4vtd17771RxwOBgHw+n5577jm9//77ysnJ0YMPPhhx2a0zLv0Z22hj0eaSCX0IoQ8h7fUhNTXVrI5evXq5XhPL9+77779vd1scD/HvQ2f37dnbqWtra+Xz+VRQUKD77rtPe/fu1cKFC5WVlaWxY8d2ejt1dXUxzSUT+hBCH0Li3YeysjLXa/773/96Xke8+9BdJEIfPAueKVOmqKysTDk5OZKkG2+8UV999ZU2b97sKnjy8/MVDAYjxvx+v+rq6qLOJRP6EEIfQtrrw2OPPRbTNisrK12v+fe//+16zbRp01yvae+Mh+Ohe/ThUg0d8Sx4fD5fOHQuKSgo0O7du11tJxgMttm09uaSCX0IoQ8h0frQ0tJitv+LFy+6XhPL962t4PnfbXI8JEYfPPsA6cqVK1VeXh4xduTIERUUFHi1CwBAD+BZ8JSVlWnv3r1au3atjh07ppdfflmvvfaaZsyY4dUuAAA9gGfBc8stt2jlypX65z//qUmTJumll17Sk08+qeLiYq92AQDoAX7QazxHjx6NeD5mzBiNGTPmBxUEIFJbH8K+NJ6RkaELFy5EzI0fPz6mfcXy2tCf//xn12s6er0GPRs3CQUAmCJ4AACmCB4AgCmCBwBgiuABAJgieAAApggeAIApggcAYIrgAQCYIngAAKYIHgCAKYIHAGCK4AEAmPLsL5AC6Bpz5syJOp6WliZJ+v3vf6/m5uaIuVj/HMn27dtdr/noo49i2heSF2c8AABTBA8AwBTBAwAwRfAAAEwRPAAAUwQPAMAUwQMAMEXwAABMETwAAFMEDwDAFMEDADBF8AAATHGTUMDQxIkTXa9ZuHBhu/OPPvroFWPfffed6/1I0hNPPBHTOsANzngAAKYIHgCAKYIHAGCK4AEAmCJ4AACmCB4AgCmCBwBgiuABAJgieAAApggeAIApggcAYIrgAQCY4iahQIz69u3res3TTz/tek2vXr1cz7/11luu9yNJu3fvjmkd4AZnPAAAUwQPAMAUwQMAMEXwAABMETwAAFMEDwDAFMEDADBF8AAATBE8AABTBA8AwBTBAwAwRfAAAExxk1BAHd+IM5rt27e7XvOTn/zE9ZpAIBB13OfzadCgQaqtrZXjOBFzCxcudL0fwApnPAAAU66Cp76+XhUVFRoxYoRKS0u1ZMkSNTc3S5KOHz+u8vJyFRUVacKECdq1a1eXFAwASGydDh7HcVRRUaGmpiZt2rRJK1as0L/+9S899dRTchxHs2bNUm5urrZt26bJkyfrkUce0YkTJ7qydgBAAur0azy1tbXav3+/PvzwQ+Xm5kqSKioqtHTpUv3sZz/T8ePHtWXLFmVmZmrQoEH6+OOPtW3bNs2ePbvLigcAJJ5On/Hk5eVpzZo14dC5pLGxUQcOHNDQoUOVmZkZHi8pKdH+/fs9KxQA0DN0+ownOztbpaWl4eetra3auHGjRo0apYaGBl133XURX9+3b1+dPHnSdUF+v7/NsWhzyYQ+hHRFH2J5V1tKis17c3w+X7vj0eazsrJi2lciHlv8XIR0hz50dt8xv526qqpKn332mV599VW9+OKLSk1NjZhPTU1VS0uL6+3W1dXFNJdM6ENIsvRh0KBB7c4XFBRcMZaMVxuS5XjoSCL0Iabgqaqq0oYNG7RixQoVFhYqLS1Np0+fjvialpYWpaenu952fn6+gsFgxJjf71ddXV3UuWRCH0K6og+xnPHs3LnT9Zri4mLXa2pra6OO+3w+FRQURP0czz333ON6P+3tqzvj5yKkO/ThUg0dcR08ixYt0ubNm1VVVaVx48ZJkvr166eampqIrzt16tQVl986IxgMttm09uaSCX0I8bIPsQRPa2urJ/vuyOWhEm3+8q9pbGyMaV+JfFzxcxGSCH1wdZH6mWee0ZYtW7R8+XJNnDgxPD58+HAdPnxY58+fD4/t27dPw4cP965SAECP0OngCQQCWrVqlWbOnKmSkhI1NDSEHyNGjFD//v01b948VVdX64UXXtDBgwc1bdq0rqwdAJCAOn2pbefOnbp48aJWr16t1atXR8wdPXpUq1at0p/+9Cfdfffduv766/Xss89qwIABnhcMAEhsPqejC8jGsrOzo7654Lvvvos6l0zoQ0hX9KGwsND1miNHjniy745Mnjw56nhGRoa2bt2q6dOnq6mpKWLujTfesCitW+DnIqQ79OFSDR3hJqEAAFMEDwDAFMEDADBF8AAATBE8AABTBA8AwBTBAwAwRfAAAEwRPAAAUwQPAMAUwQMAMEXwAABMETwAAFMx/elroLu6/vrrY1r3zjvveFxJdHPmzHG95s0334w67vf7JUnbt29P6rsyI/FwxgMAMEXwAABMETwAAFMEDwDAFMEDADBF8AAATBE8AABTBA8AwBTBAwAwRfAAAEwRPAAAUwQPAMAUNwlFj/Lwww/HtO7HP/6xx5VE995777le4zhOu+OO47T5NUB3xBkPAMAUwQMAMEXwAABMETwAAFMEDwDAFMEDADBF8AAATBE8AABTBA8AwBTBAwAwRfAAAEwRPAAAU9wkFN3W6NGjo45nZmZKkm6//XadO3cuYm727NldXheAH4YzHgCAKYIHAGCK4AEAmCJ4AACmCB4AgCmCBwBgiuABAJgieAAApggeAIApggcAYIrgAQCYIngAAKa4SSi6rdLS0qjjaWlpkqQ77rhDzc3NEXNZWVldXtclgUDA9ZrGxsYuqARILJzxAABMuQ6e+vp6VVRUaMSIESotLdWSJUvC/+pcvHixBg8eHPHYuHGj50UDABKXq0ttjuOooqJC2dnZ2rRpk86cOaP58+crJSVFc+fOVSAQUGVlpaZOnRpeY3npAwDQ/bk646mtrdX+/fu1ZMkS3XDDDbrttttUUVGhN998U1LomvfQoUOVl5cXfmRkZHRJ4QCAxOQqePLy8rRmzRrl5uZGjDc2NqqxsVH19fUaOHCgl/UBAHoYV5fasrOzI95p1Nraqo0bN2rUqFEKBALy+Xx67rnn9P777ysnJ0cPPvhgxGW3zvD7/W2ORZtLJsnWh0vvXrtcampqxH/jxefzuV5z9dVXu17T1vc72Y6HttCHkO7Qh87u2+c4jhPrTpYuXapNmzbp1Vdf1eHDhzV//nzNmTNHd9xxh/bu3aulS5dqxYoVGjt2bKy7AAD0MDEHT1VVldavX68VK1Zo3LhxchxHZ86cUU5OTvhrFi1apC+//FLr1q3r9Hbz8/MVDAYjxvx+v+rq6qLOJZNk60NlZWXU8dTUVM2bN09LlixRS0tLxNxjjz1mUZqk0Guebk2fPt31mi+++CLqeLIdD22hDyHdoQ+XauhITB8gXbRokTZv3qyqqiqNGzdOUuiyw/+GjiQVFBRo9+7drrYdDAbbbFp7c8kkWfpw+YdDL9fS0tLh13SlWP7NdvbsWddrOvpeJ8vx0BH6EJIIfXD9OZ5nnnlGW7Zs0fLlyzVx4sTw+MqVK1VeXh7xtUeOHFFBQcEPLhIA0HO4Cp5AIKBVq1Zp5syZKikpUUNDQ/hRVlamvXv3au3atTp27Jhefvllvfbaa5oxY0ZX1Q4ASECuLrXt3LlTFy9e1OrVq7V69eqIuaNHj2rlypV6+umntXLlSuXn5+vJJ59UcXGxpwUDABKbq+B5+OGH9fDDD7c5P2bMGI0ZM+YHFwUA6Lm4OzUg6cCBA67X/OIXv3C95ptvvnG9BuhpuDs1AMAUwQMAMEXwAABMETwAAFMEDwDAFMEDADBF8AAATBE8AABTBA8AwBTBAwAwRfAAAEwRPAAAW0434/f7HUkRD7/f3+ZcMj3oA32gD/ShO/fhUg0d4YwHAGCK4AEAmCJ4AACmCB4AgCmCBwBgiuABAJgieAAApggeAIApggcAYIrgAQCYIngAAKZ6x7uAy/n9/jbHos0lE/oQQh9C6EMIfQjpDn3o7L59juM4XVwLAABhXGoDAJgieAAApggeAIApggcAYIrgAQCYIngAAKYIHgCAKYIHAGCK4AEAmOr2wdPc3Kz58+frtttu0+jRo7Vu3bp4lxQX7777rgYPHhzxqKioiHdZZlpaWjRp0iTt2bMnPHb8+HGVl5erqKhIEyZM0K5du+JYoY1ofVi8ePEVx8bGjRvjWGXXqa+vV0VFhUaMGKHS0lItWbJEzc3NkpLreGivD4lwPHS7e7VdbtmyZTp06JA2bNigEydOaO7cuRowYIDGjx8f79JM1dTUqKysTIsWLQqPpaWlxbEiO83NzaqsrFR1dXV4zHEczZo1S4WFhdq2bZt27NihRx55RG+99ZYGDBgQx2q7TrQ+SFIgEFBlZaWmTp0aHsvKyrIur8s5jqOKigplZ2dr06ZNOnPmjObPn6+UlBQ9+uijSXM8tNeHuXPnJsbx4HRjZ8+edYYNG+bs3r07PPbss8869913Xxyrio/KykrnySefjHcZ5qqrq51f/epXzl133eUUFhaGj4WPPvrIKSoqcs6ePRv+2gceeMB5+umn41Vql2qrD47jOKWlpc4HH3wQx+ps1NTUOIWFhU5DQ0N47I033nBGjx6dVMdDe31wnMQ4Hrr1pbYjR47owoULKi4uDo+VlJTowIEDam1tjWNl9gKBgAYOHBjvMsx98sknGjlypLZu3RoxfuDAAQ0dOlSZmZnhsZKSEu3fv9+4Qhtt9aGxsVH19fVJcWzk5eVpzZo1ys3NjRhvbGxMquOhvT4kyvHQrS+1NTQ06Nprr1Vqamp4LDc3V83NzTp9+rT69OkTx+rsOI6jL7/8Urt27dLzzz+vixcvavz48aqoqIjoTU907733Rh1vaGjQddddFzHWt29fnTx50qIsc231IRAIyOfz6bnnntP777+vnJwcPfjggxGXWXqK7OxslZaWhp+3trZq48aNGjVqVFIdD+31IVGOh24dPE1NTVf8Yr30vKWlJR4lxcWJEyfCvXjqqaf09ddfa/HixTp//rwWLFgQ7/Lioq1jI5mOC0mqra2Vz+dTQUGB7rvvPu3du1cLFy5UVlaWxo4dG+/yulRVVZU+++wzvfrqq3rxxReT9nj43z4cPnw4IY6Hbh08aWlpVxw4l56np6fHo6S4yM/P1549e3TNNdfI5/NpyJAham1t1Zw5czRv3jz16tUr3iWaS0tL0+nTpyPGWlpakuq4kKQpU6aorKxMOTk5kqQbb7xRX331lTZv3tytftF4raqqShs2bNCKFStUWFiYtMfD5X244YYbEuJ46Nav8fTr10/ffvutLly4EB5raGhQenq6srOz41iZvZycHPl8vvDzQYMGqbm5WWfOnIljVfHTr18/nTp1KmLs1KlTV1xu6el8Pl/4l8wlBQUFqq+vj09BBhYtWqT169erqqpK48aNk5Scx0O0PiTK8dCtg2fIkCHq3bt3xAuE+/bt07Bhw5SS0q1L99QHH3ygkSNHqqmpKTz2+eefKycnJ2le57rc8OHDdfjwYZ0/fz48tm/fPg0fPjyOVdlbuXKlysvLI8aOHDmigoKC+BTUxZ555hlt2bJFy5cv18SJE8PjyXY8tNWHRDkeuvVv74yMDE2ZMkWPP/64Dh48qB07dmjdunW6//77412aqeLiYqWlpWnBggWqra3Ve++9p2XLlumhhx6Kd2lxM2LECPXv31/z5s1TdXW1XnjhBR08eFDTpk2Ld2mmysrKtHfvXq1du1bHjh3Tyy+/rNdee00zZsyId2meCwQCWrVqlWbOnKmSkhI1NDSEH8l0PLTXh4Q5HuL9fu6OnDt3znn00UedoqIiZ/To0c769evjXVJcfPHFF055eblTVFTk3Hnnnc7f/vY3p7W1Nd5lmbr88ytfffWV89vf/ta5+eabnYkTJzoffvhhHKuzc3kf3n33Xeeuu+5yhg0b5owfP955++2341hd13n++eedwsLCqA/HSZ7joaM+JMLx4HMcx4l3+AEAkke3vtQGAOh5CB4AgCmCBwBgiuABAJgieAAApggeAIApggcAYIrgAQCYIngAAKYIHgCAKYIHAGCK4AEAmPp/ATZi4lxG1aEAAAAASUVORK5CYII="
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 87
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### FashionMNIST loader setup",
   "id": "4cd484bea1c3726b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-10T20:24:29.487495Z",
     "start_time": "2024-06-10T20:24:29.480242Z"
    }
   },
   "cell_type": "code",
   "source": [
    "fashionmnist_test_loader  = torch.utils.data.DataLoader(\n",
    "    datasets.FashionMNIST('data', \n",
    "                   train=False, \n",
    "                   download=True, \n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(), \n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])\n",
    "    ),\n",
    "    batch_size=1, shuffle=True)"
   ],
   "id": "2e3b1cdffadc7d82",
   "outputs": [],
   "execution_count": 88
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-10T20:24:29.592257Z",
     "start_time": "2024-06-10T20:24:29.487495Z"
    }
   },
   "cell_type": "code",
   "source": [
    "fashionmnist_ex_img = fashionmnist_test_loader.dataset[0][0].squeeze()\n",
    "plt.imshow(fashionmnist_ex_img, cmap='gray')"
   ],
   "id": "4ec5678e0fbb00f5",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x25f24dec530>"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 89
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 3rd dataset setup",
   "id": "3a97e3170c08b420"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-10T20:24:29.595075Z",
     "start_time": "2024-06-10T20:24:29.593263Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "7a7dead43a9e85a5",
   "outputs": [],
   "execution_count": 89
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "c04df9b6e1af4d7b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## LeNet (pre-trained) model setup",
   "id": "d27ac8267aa80a72"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-10T20:24:29.599112Z",
     "start_time": "2024-06-10T20:24:29.595075Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class LeNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "        self.dropout1 = nn.Dropout(0.25)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "        self.fc1 = nn.Linear(9216, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "        output = F.log_softmax(x, dim=1)\n",
    "        return output"
   ],
   "id": "8e6a6a2e43afed95",
   "outputs": [],
   "execution_count": 90
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Pre-trained for MNIST",
   "id": "3b985ef005ad02c3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-10T20:24:29.613604Z",
     "start_time": "2024-06-10T20:24:29.599112Z"
    }
   },
   "cell_type": "code",
   "source": [
    "mnist_model = LeNet().to(device)\n",
    "mnist_model_path = r'models/lenet_mnist_model.pth'\n",
    "mnist_model.load_state_dict(torch.load(mnist_model_path, map_location=device))"
   ],
   "id": "da05efcc72d89d31",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 91
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-10T20:24:29.617303Z",
     "start_time": "2024-06-10T20:24:29.613604Z"
    }
   },
   "cell_type": "code",
   "source": "mnist_model.eval()",
   "id": "8ee95df4bfbbb08f",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LeNet(\n",
       "  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (dropout1): Dropout(p=0.25, inplace=False)\n",
       "  (dropout2): Dropout(p=0.5, inplace=False)\n",
       "  (fc1): Linear(in_features=9216, out_features=128, bias=True)\n",
       "  (fc2): Linear(in_features=128, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 92
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "4c84b3db344994aa"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Pre-trained for FashionMNIST",
   "id": "d0598d893b63372d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-10T20:24:29.653825Z",
     "start_time": "2024-06-10T20:24:29.617303Z"
    }
   },
   "cell_type": "code",
   "source": [
    "fashionmnist_model = simple_FashionMNIST(\"models/simple_FashionMNIST.pth\")\n",
    "fashionmnist_target = fashionmnist_model.model\n",
    "fashionmnist_model_device = fashionmnist_model.device\n",
    "fashionmnist_test_loader = fashionmnist_model.testloader"
   ],
   "id": "40be5542b18638ec",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on cpu.\n",
      "Model weights loaded successfully\n"
     ]
    }
   ],
   "execution_count": 93
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Some helper functions",
   "id": "2f096ce8a6bb7751"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-10T20:24:29.657901Z",
     "start_time": "2024-06-10T20:24:29.654832Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# from https://pytorch.org/tutorials/beginner/fgsm_tutorial.html\n",
    "\n",
    "def denorm(batch, mean=[0.1307], std=[0.3081]):\n",
    "    \"\"\"\n",
    "    Convert a batch of tensors to their original scale.\n",
    "\n",
    "    Args:\n",
    "        batch (torch.Tensor): Batch of normalized tensors.\n",
    "        mean (torch.Tensor or list): Mean used for normalization.\n",
    "        std (torch.Tensor or list): Standard deviation used for normalization.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: batch of tensors without normalization applied to them.\n",
    "    \"\"\"\n",
    "    if isinstance(mean, list):\n",
    "        mean = torch.tensor(mean).to(device)\n",
    "    if isinstance(std, list):\n",
    "        std = torch.tensor(std).to(device)\n",
    "\n",
    "    return batch * std.view(1, -1, 1, 1) + mean.view(1, -1, 1, 1)"
   ],
   "id": "38b3c2a5fd47d18f",
   "outputs": [],
   "execution_count": 94
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-10T20:24:29.666744Z",
     "start_time": "2024-06-10T20:24:29.658907Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# MODIFED from https://pytorch.org/tutorials/beginner/fgsm_tutorial.html\n",
    "# changelog:\n",
    "# added algo parameter to the function signature and call to the algo function as to define test funct only once and call it with the desired attack algo\n",
    "# added alpha parameter to the function signature as to define test funct only once and call it with the desired step size\n",
    "# added num_iter parameter to the function signature\n",
    "# adapted function returns to fit running function\n",
    "# if statement for FSGM (needs some tinkering cuz the code is too long)\n",
    "\n",
    "def test(model, device, test_loader, epsilon, algo, alpha, num_iter):\n",
    "    if algo == 'FSGM_attack':\n",
    "        correct = 0\n",
    "        adv_examples = []\n",
    "\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            data.requires_grad = True\n",
    "            output = model(data)\n",
    "            init_pred = output.max(1, keepdim=True)[1]\n",
    "\n",
    "            if init_pred.item() != target.item():\n",
    "                continue\n",
    "\n",
    "            loss = F.nll_loss(output, target)\n",
    "            model.zero_grad()\n",
    "            loss.backward()\n",
    "            data_grad = data.grad.data\n",
    "            data_denorm = denorm(data)\n",
    "\n",
    "            perturbed_data = FSGM_attack(data_denorm, epsilon, data_grad)\n",
    "\n",
    "            perturbed_data_normalized = transforms.Normalize((0.1307,), (0.3081,))(perturbed_data)\n",
    "            output = model(perturbed_data_normalized)\n",
    "\n",
    "            final_pred = output.max(1, keepdim=True)[1]\n",
    "            if final_pred.item() == target.item():\n",
    "                correct += 1\n",
    "                if epsilon == 0 and len(adv_examples) < 5:\n",
    "                    adv_ex = perturbed_data.squeeze().detach().cpu().numpy()\n",
    "                    adv_examples.append((init_pred.item(), final_pred.item(), adv_ex))\n",
    "            else:\n",
    "                if len(adv_examples) < 5:\n",
    "                    adv_ex = perturbed_data.squeeze().detach().cpu().numpy()\n",
    "                    adv_examples.append((init_pred.item(), final_pred.item(), adv_ex))\n",
    "\n",
    "        final_acc = correct / float(len(test_loader))\n",
    "        print(f\"Epsilon: {epsilon}\\tTest Accuracy = {correct} / {len(test_loader)} = {final_acc}\")\n",
    "\n",
    "        return final_acc, adv_examples\n",
    "\n",
    "    else:\n",
    "        correct = 0\n",
    "        adv_examples = []\n",
    "        iteration_accuracies = []\n",
    "        step_sizes = []\n",
    "        iteration_times = []\n",
    "\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            data.requires_grad = True\n",
    "            output = model(data)\n",
    "            init_pred = output.max(1, keepdim=True)[1]\n",
    "\n",
    "            if init_pred.item() != target.item():\n",
    "                continue\n",
    "\n",
    "            loss = F.nll_loss(output, target)\n",
    "            model.zero_grad()\n",
    "            loss.backward()\n",
    "            data_grad = data.grad.data\n",
    "            data_denorm = denorm(data)\n",
    "\n",
    "            start_time = time.time()\n",
    "            if algo == PGD_const:\n",
    "                perturbed_data = PGD_const(model, data_denorm, target, epsilon, alpha, num_iter)\n",
    "            else:\n",
    "                perturbed_data = algo(model, data_denorm, target, epsilon, num_iter, alpha)\n",
    "            iteration_time = time.time() - start_time\n",
    "\n",
    "            perturbed_data_normalized = transforms.Normalize((0.1307,), (0.3081,))(perturbed_data)\n",
    "            output = model(perturbed_data_normalized)\n",
    "            final_pred = output.max(1, keepdim=True)[1]\n",
    "\n",
    "            if final_pred.item() == target.item():\n",
    "                correct += 1\n",
    "                if epsilon == 0 and len(adv_examples) < 5:\n",
    "                    adv_ex = perturbed_data.squeeze().detach().cpu().numpy()\n",
    "                    adv_examples.append((init_pred.item(), final_pred.item(), adv_ex))\n",
    "            else:\n",
    "                if len(adv_examples) < 5:\n",
    "                    adv_ex = perturbed_data.squeeze().detach().cpu().numpy()\n",
    "                    adv_examples.append((init_pred.item(), final_pred.item(), adv_ex))\n",
    "\n",
    "            iteration_accuracies.append(correct / float(len(test_loader)))\n",
    "            step_sizes.append(epsilon)\n",
    "            iteration_times.append(iteration_time)\n",
    "\n",
    "        final_acc = correct / float(len(test_loader))\n",
    "        print(f\"Epsilon: {epsilon}\\tTest Accuracy = {correct} / {len(test_loader)} = {final_acc}\")\n",
    "\n",
    "        return final_acc, adv_examples, iteration_accuracies, step_sizes, iteration_times"
   ],
   "id": "7fede79e9040c15b",
   "outputs": [],
   "execution_count": 95
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-10T20:24:29.674444Z",
     "start_time": "2024-06-10T20:24:29.666744Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def plot_metrics(epsilons, accuracies_dict, iteration_accuracies_dict, iteration_times_dict, step_sizes_dict):\n",
    "    fig, axs = plt.subplots(4, 1, figsize=(10, 24))\n",
    "\n",
    "    # Accuracy vs Epsilon\n",
    "    for dataset_name, accuracies in accuracies_dict.items():\n",
    "        sns.lineplot(ax=axs[0], x=epsilons, y=accuracies, label=f\"{dataset_name}\", linewidth=0.75, errorbar=None)\n",
    "    axs[0].set_xlabel('Epsilon', fontsize=14)\n",
    "    axs[0].set_ylabel('Accuracy', fontsize=14)\n",
    "    axs[0].set_title(\"Accuracy vs Epsilon\", fontsize=16)\n",
    "    axs[0].legend()\n",
    "\n",
    "    # Accuracy vs Iteration\n",
    "    for dataset_name, iteration_accuracies in iteration_accuracies_dict.items():\n",
    "        sns.lineplot(ax=axs[1], x=np.arange(len(iteration_accuracies)), y=iteration_accuracies, label=f\"{dataset_name}\")\n",
    "    axs[1].set_title('Accuracy vs Iteration')\n",
    "    axs[1].set_xlabel('Iteration')\n",
    "    axs[1].set_ylabel('Accuracy')\n",
    "    axs[1].legend()\n",
    "\n",
    "    # Time vs Iteration as histogram\n",
    "    for dataset_name, iteration_times in iteration_times_dict.items():\n",
    "        sns.histplot(ax=axs[2], x=np.arange(len(iteration_times)), weights=iteration_times, bins=len(iteration_times), kde=True, label=f\"{dataset_name}\")\n",
    "    axs[2].set_title('Time vs Iteration')\n",
    "    axs[2].set_xlabel('Iteration')\n",
    "    axs[2].set_ylabel('Time (s)')\n",
    "    axs[2].legend()\n",
    "\n",
    "    # Step Size Behavior\n",
    "    for dataset_name, step_sizes in step_sizes_dict.items():\n",
    "        sns.lineplot(ax=axs[3], x=np.arange(len(step_sizes)), y=step_sizes, label=f\"{dataset_name}\")\n",
    "    axs[3].set_title('Step Size Behavior')\n",
    "    axs[3].set_xlabel('Iteration')\n",
    "    axs[3].set_ylabel('Step Size')\n",
    "    axs[3].legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ],
   "id": "e5ea74b9e012e6bb",
   "outputs": [],
   "execution_count": 96
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-10T20:24:29.681955Z",
     "start_time": "2024-06-10T20:24:29.674444Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def history(epsilons, accuracies_dict, iteration_accuracies_dict, iteration_times_dict, step_sizes_dict):\n",
    "    import pandas as pd\n",
    "    metrics = []\n",
    "    for dataset_name in accuracies_dict.keys():\n",
    "        for i, epsilon in enumerate(epsilons):\n",
    "            metrics.append({\n",
    "                \"Dataset\": dataset_name,\n",
    "                \"Epsilon\": epsilon,\n",
    "                \"Accuracy\": accuracies_dict[dataset_name][i],\n",
    "                \"Iteration_Accuracy\": iteration_accuracies_dict[dataset_name][i] if i < len(iteration_accuracies_dict[dataset_name]) else None,\n",
    "                \"Iteration_Time\": iteration_times_dict[dataset_name][i] if i < len(iteration_times_dict[dataset_name]) else None,\n",
    "                \"Step_Size\": step_sizes_dict[dataset_name][i] if i < len(step_sizes_dict[dataset_name]) else None,\n",
    "            })\n",
    "    metrics_df = pd.DataFrame(metrics)\n",
    "    tools.display_dataframe_to_user(name=\"Attack Metrics\", dataframe=metrics_df)"
   ],
   "id": "c4edf04b542e64bf",
   "outputs": [],
   "execution_count": 97
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-10T20:24:29.688597Z",
     "start_time": "2024-06-10T20:24:29.681955Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# MODIFED from https://pytorch.org/tutorials/beginner/fgsm_tutorial.html\n",
    "# changelog: made it a function instead of the whole code so that we slim the code\n",
    "\n",
    "def plot_examples(examples, epsilons):\n",
    "    cnt = 0\n",
    "    plt.figure(figsize=(8, 10))\n",
    "    for i in range(len(epsilons)):\n",
    "        for j in range(len(examples[i])):\n",
    "            cnt += 1\n",
    "            plt.subplot(len(epsilons), len(examples[0]), cnt)\n",
    "            plt.xticks([], [])\n",
    "            plt.yticks([], [])\n",
    "            if j == 0:\n",
    "                plt.ylabel(f\"Eps: {epsilons[i]}\", fontsize=14)\n",
    "            orig, adv, ex = examples[i][j]\n",
    "            plt.title(f\"{orig} -> {adv}\")\n",
    "            plt.imshow(ex, cmap=\"gray\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ],
   "id": "927ac72f3d07fb",
   "outputs": [],
   "execution_count": 98
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-10T20:24:29.694943Z",
     "start_time": "2024-06-10T20:24:29.688597Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model_dict = {\n",
    "    \"MNIST\": mnist_model,\n",
    "    \"FashionMNIST\": fashionmnist_model,\n",
    "    # Add 3rd dataset model here\n",
    "}\n",
    "\n",
    "test_loader_dict = {\n",
    "    \"MNIST\": mnist_test_loader,\n",
    "    \"FashionMNIST\": fashionmnist_test_loader,\n",
    "    # Add 3rd dataset loader here\n",
    "}"
   ],
   "id": "947e6937e9280460",
   "outputs": [],
   "execution_count": 99
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-10T20:24:29.701968Z",
     "start_time": "2024-06-10T20:24:29.694943Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# function runs the attack for each epsilon and plots the accuracy, and at the end plots the examples\n",
    "\n",
    "def RUN(model_dict, device, test_loader_dict, algo, num_iter, epsilons, alpha):\n",
    "    accuracies_dict, examples_dict = {}, {}\n",
    "    all_iteration_accuracies_dict, all_step_sizes_dict, all_iteration_times_dict = {}, {}, {}\n",
    "\n",
    "    for dataset_name, model in model_dict.items():\n",
    "        test_loader = test_loader_dict[dataset_name]\n",
    "        accuracies, examples, all_iteration_accuracies, all_step_sizes, all_iteration_times = [], [], [], [], []\n",
    "\n",
    "        for eps in epsilons:\n",
    "            acc, ex, iteration_accuracies, step_sizes, iteration_times = test(model, device, test_loader, eps, algo, alpha, num_iter)\n",
    "            accuracies.append(acc)\n",
    "            examples.append(ex)\n",
    "            all_iteration_accuracies.extend(iteration_accuracies)\n",
    "            all_step_sizes.extend(step_sizes)\n",
    "            all_iteration_times.extend(iteration_times)\n",
    "\n",
    "        accuracies_dict[dataset_name] = accuracies\n",
    "        examples_dict[dataset_name] = examples\n",
    "        all_iteration_accuracies_dict[dataset_name] = all_iteration_accuracies\n",
    "        all_step_sizes_dict[dataset_name] = all_step_sizes\n",
    "        all_iteration_times_dict[dataset_name] = all_iteration_times\n",
    "\n",
    "    plot_metrics(epsilons, accuracies_dict, all_iteration_accuracies_dict, all_iteration_times_dict, all_step_sizes_dict)\n",
    "    history(epsilons, accuracies_dict, all_iteration_accuracies_dict, all_iteration_times_dict, all_step_sizes_dict)"
   ],
   "id": "47b2df81ad7a564",
   "outputs": [],
   "execution_count": 100
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Optimization ''hyperparams'' setup",
   "id": "206e2997e467b290"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Perturbation set bounds",
   "id": "61bcd3567664962a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-10T20:24:29.708241Z",
     "start_time": "2024-06-10T20:24:29.701968Z"
    }
   },
   "cell_type": "code",
   "source": "epsilons = [0, .05, .1, .15, .2, .25, .3, .35]",
   "id": "9aa8b4be7842190c",
   "outputs": [],
   "execution_count": 101
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Number of iterations",
   "id": "d121465081ed304c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-10T20:24:29.714653Z",
     "start_time": "2024-06-10T20:24:29.708241Z"
    }
   },
   "cell_type": "code",
   "source": "num_iter = 5",
   "id": "778109563600441f",
   "outputs": [],
   "execution_count": 102
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Constant step sizes",
   "id": "c6f8affa9c2f6510"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Unit step size",
   "id": "f93ab77fe3a82a45"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-10T20:24:29.722981Z",
     "start_time": "2024-06-10T20:24:29.715658Z"
    }
   },
   "cell_type": "code",
   "source": "alpha_unit = 1",
   "id": "4866de30cfdee6fc",
   "outputs": [],
   "execution_count": 103
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Lipschitz constant dependent step size",
   "id": "6b1a0fc3b4e00056"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-10T20:24:29.786548Z",
     "start_time": "2024-06-10T20:24:29.723985Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def lipschitz_constant(model):\n",
    "    L = 0  # initialize\n",
    "\n",
    "    for layer in model.children():\n",
    "        if isinstance(layer, nn.Conv2d) or isinstance(layer, nn.Linear):\n",
    "            \n",
    "            weight = layer.weight.data # extract weight data\n",
    "            \n",
    "            singular_values = torch.svd(weight).S     # as we did initially in the homework, we approximate the Lipschitz\n",
    "            L = max(L, singular_values.max().item())  # constant as the maximum singular value of the weight matrix\n",
    "    \n",
    "    return L\n",
    "\n",
    "L_mnist = lipschitz_constant(mnist_model)\n",
    "L_Fmnist = lipschitz_constant(fashionmnist_target)\n",
    "\n",
    "alpha_lipschitz_mnist = 1 / L_mnist\n",
    "alpha_lipschitz_Fmnist = 1 / L_Fmnist\n",
    "\n",
    "print(f\"MNIST: {alpha_lipschitz_mnist}\", f\"FashionMNIST: {alpha_lipschitz_Fmnist}\")"
   ],
   "id": "fe6d41e8919da337",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MNIST: 0.14922538993069884 FashionMNIST: 0.051946399758641376\n"
     ]
    }
   ],
   "execution_count": 104
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Adaptive step sizes",
   "id": "5a4240a4b87ad7db"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Diminishing step size",
   "id": "6829a2823d146635"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-10T20:24:29.789858Z",
     "start_time": "2024-06-10T20:24:29.787554Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def alpha_dim(num_iter):\n",
    "    alphas = [2 / (k + 2) for k in range(num_iter)] # defined as a list cuz it's easier to call the i-th element\n",
    "    return alphas"
   ],
   "id": "7c1a6166133a94b2",
   "outputs": [],
   "execution_count": 105
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "##### Exact line search\n",
    "\n",
    "From Rinaldi's survey paper:\n",
    "\n",
    "The exact line search method aims to find the optimal step size $ \\( \\alpha \\)$  that minimizes the objective function along the direction of the gradient descent.\n",
    "\n",
    "$ \\[ \\alpha_k = \\min_{\\alpha \\in [0, \\alpha_{\\text{max}}^k]} \\varphi(\\alpha) \\] $, where:\n",
    "\n",
    "$ \\[ \\varphi(\\alpha) = f(x_k + \\alpha d_k) \\]$ \n",
    "\n",
    "- $\\( x_k \\)$: Current point in the parameter space.\n",
    "- $\\( d_k \\)$: Descent direction (typically the negative gradient).\n",
    "- $\\( \\alpha_{\\text{max}}^k \\)$: Maximum allowable step size.\n",
    "- $\\( \\varphi(\\alpha) \\)$: Objective function along the direction $\\( d_k \\)$ with step size $\\( \\alpha \\)$.\n",
    "\n",
    "The goal is to find the smallest $\\( \\alpha \\)$ that minimizes $\\( \\varphi(\\alpha) \\)$."
   ],
   "id": "6cf0aba7550f4534"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-10T20:24:29.802460Z",
     "start_time": "2024-06-10T20:24:29.800379Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# define initial step size\n",
    "\n",
    "alpha_max = 0.01 # for adversarial attacks on images with pixel values normalized between 0 and 1, α_max = 0.1 or α_max = 0.01 is a common choice."
   ],
   "id": "165878b37ac4fd55",
   "outputs": [],
   "execution_count": 106
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-10T20:24:29.877451Z",
     "start_time": "2024-06-10T20:24:29.873559Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def exact_LS(model, image, target, data_grad, num_iter):\n",
    "    def objective_function(alpha):\n",
    "        perturbed_image = image + alpha * data_grad\n",
    "        perturbed_image = torch.clamp(perturbed_image, 0, 1)\n",
    "        output = model(perturbed_image)\n",
    "        loss = F.cross_entropy(output, target)\n",
    "        return loss.item()\n",
    "\n",
    "    alpha_range = torch.linspace(0, alpha_max, num_iter)\n",
    "    losses = [objective_function(alpha) for alpha in alpha_range]\n",
    "    best_alpha = alpha_range[torch.argmin(torch.tensor(losses))]\n",
    "\n",
    "    return best_alpha"
   ],
   "id": "5d7328428576fe36",
   "outputs": [],
   "execution_count": 107
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "##### Armijo rule\n",
    "\n",
    "From Rinaldi's survey paper:\n",
    "\n",
    "\n",
    "$\\[ \\alpha_k = \\min_{\\alpha \\in [0, \\alpha_{\\text{max}}^k]} \\varphi(\\alpha) \\]$, where:\n",
    "\n",
    "$\\[ \\varphi(\\alpha) = f(x_k + \\alpha d_k) \\]$\n",
    "\n",
    "- $\\( x_k \\)$: Current point in the parameter space.\n",
    "- $\\( d_k \\)$: Descent direction (typically the negative gradient).\n",
    "- $\\( \\alpha_{\\text{max}}^k \\)$: Maximum allowable step size.\n",
    "- $\\( \\varphi(\\alpha) \\)$: Objective function along the direction $\\( d_k \\)$ with step size $\\( \\alpha \\)$.\n",
    "\n",
    "The goal is to find the smallest $\\( \\alpha \\)$ that minimizes $\\( \\varphi(\\alpha) \\)$."
   ],
   "id": "170e6af1a0538544"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-10T20:24:29.907144Z",
     "start_time": "2024-06-10T20:24:29.903598Z"
    }
   },
   "cell_type": "code",
   "source": [
    "delta = 0.5\n",
    "gamma = 0.1\n",
    "M = 1000 \n",
    "\n",
    "def armijo(model, image, target, data_grad):\n",
    "    alpha = alpha_max\n",
    "    m = 0\n",
    "\n",
    "    while m < M:\n",
    "        perturbed_image = image + alpha * data_grad\n",
    "        perturbed_image = torch.clamp(perturbed_image, 0, 1)\n",
    "        output = model(perturbed_image)\n",
    "        loss = F.cross_entropy(output, target)\n",
    "        \n",
    "        if loss.item() <= F.cross_entropy(model(image), target).item() + gamma * alpha * data_grad.norm():\n",
    "            break\n",
    "        else:\n",
    "            alpha *= delta\n",
    "            m += 1\n",
    "\n",
    "    return alpha"
   ],
   "id": "868dbb5fa091c26d",
   "outputs": [],
   "execution_count": 108
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "1ceafc6147155327"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Non-FW Attacks",
   "id": "c99182d5c90c8340"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Attack PGD",
   "id": "7af2bc044aaed00e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-10T20:24:29.941072Z",
     "start_time": "2024-06-10T20:24:29.938681Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# could not slim the code to only one function\n",
    "# as for constant step sizes, the attack follows an algo structure that requires a function arg that should not be passed in adaptive step sizes algos\n",
    "# so at most I could define two functions for each algo, one for constant step sizes and one for adaptive step sizes"
   ],
   "id": "b5f03216d529fa53",
   "outputs": [],
   "execution_count": 109
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-10T20:24:31.315795Z",
     "start_time": "2024-06-10T20:24:31.311911Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def PGD_const(model, image, target, epsilon, alpha, num_iter):\n",
    "    perturbed_image = image.clone().detach().requires_grad_(True)\n",
    "    \n",
    "    # call the constant step size\n",
    "    if alpha == 'alpha_unit':\n",
    "        best_alpha = alpha_unit\n",
    "    else: # if we call alpha_lipschitz\n",
    "        if model == mnist_model:\n",
    "            best_alpha = alpha_lipschitz_mnist\n",
    "        elif model == fashionmnist_model:\n",
    "             best_alpha = alpha_lipschitz_Fmnist\n",
    "            \n",
    "    for _ in range(num_iter):\n",
    "        # Zero all existing gradients\n",
    "        perturbed_image.requires_grad = True\n",
    "\n",
    "        # Forward pass the data through the model\n",
    "        output = model(perturbed_image)\n",
    "        loss = torch.nn.functional.cross_entropy(output, target)\n",
    "            \n",
    "        # Zero all existing gradients\n",
    "        model.zero_grad()\n",
    "\n",
    "        # Calculate gradients of model in backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Collect the element-wise sign of the data gradient\n",
    "        data_grad = perturbed_image.grad.data\n",
    "        \n",
    "\n",
    "        perturbed_image = perturbed_image + best_alpha * data_grad.sign() # move in the direction of the gradient\n",
    "        perturbation = torch.clamp(perturbed_image - image, -epsilon, epsilon) # clip to stay within the given epsilon\n",
    "        perturbed_image = torch.clamp(image + perturbation, 0, 1).detach() # apply perturbation and clip within valid pixel range (we de-normed to [0,1])\n",
    "\n",
    "    # Return the final perturbed image after all iterations\n",
    "    return perturbed_image"
   ],
   "id": "107108c598b3f8f",
   "outputs": [],
   "execution_count": 110
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-10T20:24:33.044105Z",
     "start_time": "2024-06-10T20:24:33.039892Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def PGD_adapt(model, image, target, epsilon, num_iter, search_method):\n",
    "    perturbed_image = image.clone().detach().requires_grad_(True)\n",
    "\n",
    "    for i in range(num_iter):\n",
    "        perturbed_image.requires_grad = True\n",
    "\n",
    "        output = model(perturbed_image)\n",
    "        loss = F.cross_entropy(output, target)\n",
    "\n",
    "        model.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        data_grad = perturbed_image.grad.data\n",
    "\n",
    "        if search_method == \"exact\":\n",
    "            best_alpha = exact_LS(model, perturbed_image, target, data_grad, num_iter)\n",
    "        elif search_method == \"armijo\":\n",
    "            best_alpha = armijo(model, perturbed_image, target, data_grad)\n",
    "        elif search_method == \"alpha_dim\":\n",
    "            alphas = alpha_dim(num_iter)\n",
    "            best_alpha = alphas[i] # # use the precomputed alpha value (since in this case it's only dependent on the number of iterations)\n",
    "\n",
    "        perturbed_image = perturbed_image + best_alpha * data_grad.sign() # move in the direction of the gradient\n",
    "        perturbation = torch.clamp(perturbed_image - image, -epsilon, epsilon) # clip to stay within the given epsilon\n",
    "        perturbed_image = torch.clamp(image + perturbation, 0, 1).detach() # apply perturbation and clip within valid pixel range (we de-normed to [0,1])\n",
    "\n",
    "    return perturbed_image"
   ],
   "id": "781757aa0b8f7155",
   "outputs": [],
   "execution_count": 111
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Attack PGD unit step size",
   "id": "daeca8507f42543"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "RUN(model_dict, device, test_loader_dict, PGD_const, num_iter, epsilons, 'alpha_unit')",
   "id": "6179f2f710bcc386",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Attack PGD lipschitz constant dependent step size",
   "id": "f0e10ca4b831d6b3"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2024-06-10T20:24:34.991286Z"
    }
   },
   "cell_type": "code",
   "source": "RUN(model_dict, device, test_loader_dict, PGD_const, num_iter, epsilons, 'alpha_lipschitz')",
   "id": "aa55a7b0152a0c1e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Attack PGD diminishing step size",
   "id": "549a639a8b640967"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "RUN(model_dict, device, test_loader_dict, PGD_adapt, num_iter, \"alpha_dim\")",
   "id": "bb8103f35f1224f5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Attack PGD exact line search",
   "id": "37625475a2d7a242"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "RUN(model_dict, device, test_loader_dict, PGD_adapt, num_iter, \"exact\")",
   "id": "8683a50f0d893837",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Attack PGD armijo rule",
   "id": "74e2b3c869f65b6c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "RUN(model_dict, device, test_loader_dict, PGD_adapt, num_iter, \"armijo\")",
   "id": "b1627d3f244153c0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Attack FSGM",
   "id": "2afc0e38ff39b481"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# MODIFIED from https://pytorch.org/tutorials/beginner/fgsm_tutorial.html\n",
    "# no step size since it is not an iterative method\n",
    "# changelog:\n",
    "# added model, target, data_grad, alpha and num_iter parameters to the function signature to fit it to the test function\n",
    "\n",
    "# FGSM attack code\n",
    "def FSGM_attack(image, epsilon, data_grad):\n",
    "    # Collect the element-wise sign of the data gradient\n",
    "    sign_data_grad = data_grad.sign()\n",
    "    # Create the perturbed image by adjusting each pixel of the input image\n",
    "    perturbed_image = image + epsilon*sign_data_grad\n",
    "    # Adding clipping to maintain [0,1] range\n",
    "    perturbed_image = torch.clamp(perturbed_image, 0, 1)\n",
    "    # Return the perturbed image\n",
    "    return perturbed_image"
   ],
   "id": "7832b9eb42cbf0f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# since it is not an iterative model, we can't plot time per iteration and accuracy per iteration metrics, so we'll use raw code\n",
    "# MODIFIED from https://pytorch.org/tutorials/beginner/fgsm_tutorial.html\n",
    "# changelog:\n",
    "# fit the function to the test function's if statement, with of course None as step size and number of iterations\n",
    "## test function may be further slimmed by avoiding some repetition putting the if statement somewhere else, but it's not a big deal\n",
    "\n",
    "accuracies = []\n",
    "examples = []\n",
    "\n",
    "# Run test for each epsilon\n",
    "def RUN_FSGM(model, device, test_loader, epsilons):\n",
    "    for eps in epsilons:\n",
    "        acc, ex = test(model, device, test_loader, eps, 'FSGM_attack', None, None)\n",
    "        accuracies.append(acc)\n",
    "        examples.append(ex)\n",
    "        \n",
    "    plt.figure(figsize=(5,5))\n",
    "    plt.plot(epsilons, accuracies, \"*-\")\n",
    "    plt.yticks(np.arange(0, 1.1, step=0.1))\n",
    "    plt.xticks(np.arange(0, .35, step=0.05))\n",
    "    plt.title(\"Accuracy vs Epsilon\")\n",
    "    plt.xlabel(\"Epsilon\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.show()    \n",
    "    \n",
    "    plot_examples(examples, epsilons)\n",
    "    \n",
    "RUN_FSGM(model, device, test_loader, epsilons)"
   ],
   "id": "72e4fb09193933b4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# FW (and variants) Attacks",
   "id": "2ff1f6b9a19b01a5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "# merge tanner code",
   "id": "55103f50f9b37072",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
